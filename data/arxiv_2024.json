[
    {
        "text": "We proposed the Randomized Embedding Smoothing and Token Aggregation (RESTA) defense, which adds noise to the embedding vectors and aggregates during token generation. Our experimental results provide an initial proof-of-concept that demonstrated the effectiveness of RESTA to reduce the ASR of jailbreaking attacks, while maintaining model utility, and explored the effect of embedding perturbation direction. As many jailbreaking attacks and defense methods have been recently emerging, we will conduct further experiments to compare with other methods in our future work."
    },
    {
        "text": "In this paper, we introduce a safety realignment method IRR, which enhances model safety by identifying and removing unsafe delta parameters while recalibrating the remaining ones. IRR significantly improves the effectiveness of safety realignment. Evaluations on various harmful query benchmarks and jailbreak attacks indicate that IRR considerably reduces the risks of fine-tuned models. Among various fine-tuning methods, datasets, and models, IRR outperforms baseline methods by improving model safety while maintaining downstream task performance, achieving Pareto improvements."
    },
    {
        "text": "In this technical report, we introduce DeepSeek-VL2, an enhanced version of MoE-based VisionLanguage Models, available in scales of 3B, 16B, and 27B parameters in total, with corresponding activated parameters of 1.0B, 2.8B, and 4.5B. This configuration facilitates efficient computational consumption during both training and inference stages. Notably, our 3B, 16B and 27B models can be deployed on a single GPU with 10 GB, 40GB and 80GB memory respectively. We employ a dynamic tiling vision encoding strategy to efficiently process high-resolution images with various aspect ratios. By making codes and pre-trained models publicly available, we aim to stimulate further advancements and applications at the intersection of vision and language."
    },
    {
        "text": "AD-LLM explores the application of LLMs in enhancing AD through zero-shot detection, data augmentation, and model selection. These contributions have the potential to significantly improve real-world AD systems in critical areas such as healthcare, finance, and cybersecurity. By enabling robust, adaptable, and efficient solutions for AD tasks, this research empowers practitioners to deploy systems that are responsive to novel challenges while reducing reliance on labeled data and extensive domain expertise."
    },
    {
        "text": "Aligning large language models to human values is an extremely complex problem and requires to consider very diverse aspects, cultures, regions and countries. Existing works mainly focus on single perspective, however, modeling very different perspectives such as values, safety, social norms, and ethics in one paradigm is yet to be consider. Therefore, it is indispensable to design multi-objective optimization alignment algorithms to model these factors simultaneously. This is quite challenging since these social perspectives are interleaved together and have different meanings in different contexts (such as cultures, countries or regions)."
    },
    {
        "text": "Galactic halos are far more mysterious and difficult to study than galactic disks. One thing from the flat rotational curves of the galaxies is sure that there is more matter outside the galactic disks, but the nature of this matter is still unknown. HVCs populate the galactic halos in large quantities but the exact distances of these clouds are still not estimated, since there are only a few early type stars in the halos to do spectroscopic measurements."
    },
    {
        "text": "In this paper, we propose an iterative distributed computing estimator for the multinomial logistic model that is fast to compute even when the number of choices is large. When the number of iterations goes to infinity, we show that our estimator is both consistent and asymptotically efficient. Based on the simulation study, the computational time of our estimator increases linearly with the number of choices. Moreover, our estimator has comparable finite sample performance to MLE when the latter is computationally feasible. These modifications to the IDC estimator have the potential to further enhance computational efficiency, depending on the application. However, their theoretical properties require further investigation. Second, the asymptotic properties of the IDC estimator in this paper are established for a large but fixed number of choices."
    },
    {
        "text": "This paper is the first to study the role of performance standards in rank-order tournaments. Standards play an important role in tournament design. In various settings with competitive incentives, such as the assignment of raises, bonuses or promotions in organizations, or innovation contests, it is in the principal’s interest to not only reward the best performer(s) but also to ensure that the performance of those selected exceeds a minimum standard. We show that the optimal standard is always at a mode of the performance distribution, and provide a condition for when it is at the global mode. We find optimal prize schemes—to be used together with the optimal standard—for distributions with monotone hazard rates and also show that when the noise distribution is log-concave or log-convex, a tournament with the optimal standard and prize schedule achieves optimality in a much wider class of monotone, symmetric pay schemes based on cardinal performance."
    },
    {
        "text": "This paper shows that the estimation of linear models with endogeneity using the popular twostages least squares (2SLS) method may not be estimating the parameter of interest. In particular, in the presence of heteroskedasticity, something common when correcting the variance covariance matrix and for doing correct inference, the 2SLS is inconsistent. The empirical literature has not paid specific attention to this issue, probably because the interest lied in estimating the LATE rather than the ATE, as popularized by the treatment effects agenda. As an alternative we provide a simple to implement augmented control function estimator. The control function (CF) approach provides an intuitive modeling strategy to handle endogeneity (see Wooldridge (2015) for a review) and lies in the center of general approaches tothis particular set-up (see Newey, Powell, and Vella (1999) and Florens, Heckman, Meghir, and Vytlacil (2008)). We show that this can be adapted to the least-squares linear estimation methods to correct for the 2SLS bias. In fact, the proposed solution is to augment the standard CF model (which corresponds to 2SLS) with interactions of the structural equation variables with the residuals from the first-stage. An important future research is the extension of the methods developed in this paper to study panel data models."
    },
    {
        "text": "This study aimed to explore the effects of e-commerce in African regions and to develop a readiness score system that quantifies the potential for e-commerce growth. Through a thorough case study and an analysis of GDP per capita, mobile money deployments, internet penetration, and population, this research provides actionable insights into the factors enabling or hindering e-commerce adoption. The e-commerce readiness analysis, supported by a scoring model, showed significant disparities among African countries. High-scoring nations like Nigeria, Kenya, and South Africa demonstrated the impact of robust digital infrastructure, mobile money options, and economic growth on providing e-commerce ecosystems. Conversely, lowerscoring countries such as Chad and the Central African Republic underscored persistent challenges posed by inadequate infrastructure, limited financial inclusion, and low internet access. These emphasize the importance of addressing foundational gaps to foster economic growth across regions. Our case studies further contextualize these insights, showing how e-commerce has reshaped economies at both urban and rural areas. In cities like Lagos, Nairobi, Johannesburg, and Cape Town, e-commerce has boosted small and medium-sized enterprises (SMEs), increased employment opportunities, and enhanced operational efficiencies. These urban areas benefit from relatively advanced digital infrastructure and higher levels of digital literacy, facilitating smoother adoption of e-commerce platforms. Meanwhile, rural regions such as Kano, Kaduna, and the Rift Valley revealed the challenges of limited internet access, low digital literacy, and inadequate infrastructure, which hinder e-commerce’s full potential despite localized successes like mobile money systems and agricultural platforms. The empirical analysis underscored the disparities in e-commerce impacts between urban and rural economies. While urban areas experienced many improvements in efficiency, profitability, and market access, rural areas continued to grapple with barriers that limit participation in the digital economy. "
    },
    {
        "text": "This paper introduces a novel procedure for predicting large integrated volatility matrices using high-frequency financial data. The proposed PT-POET method leverages daily volatility dynamics based on the semiparametric structure of the low-rank tensor component of the integrated volatility matrix process. We establish the asymptotic properties of PT-POET and its estimator for the future integrated volatility matrix. In the empirical study, PT-POET outperforms conventional methods in terms of out-of-sample performance for predicting the one-day-ahead integrated volatility matrix and portfolio allocation. This finding confirms that generalizing the low-rank structure and incorporating the HAR model structure into interday volatility dynamics improves the prediction of future volatility matrices. We note that, in this paper, we developed a generalized model for large volatility matrix processes and utilized the past leading eigenvalues as covariates for projecting the dynamics of singular vectors. However, it would be an interesting direction for future research to explore other possible covariates, such as trading volume, with alternative basis functions. This study requires extensive empirical research. Thus, we leave this for future research."
    },
    {
        "text": "This study undertook a state-level analysis of historical carbon emissions and decarbonization efforts concerning residential space cooling in India. The comprehensive analysis encompassed both urban and rural households across all 33 states and territories of India from 2000 to 2022. Initially, a micro-level investigation of residential space cooling activities was conducted using the DREAM approach, examining the historical evolution of the corresponding carbon intensity. Subsequently, a macro-level exploration based on the DSD method delved into the societal, economic, climatic, and technological factors influencing changes in carbon intensity for India’s residential space cooling. Furthermore, the historical performance of Indian states in decarbonizing urban and rural residential space cooling across four emission scales was examined. Finally, the effectiveness of initiated residential space cooling policies was discussed to identify optimal paths for expediting future decarbonization efforts."
    },
    {
        "text": "This paper presents a novel film volume model that integrates frost and retained water/ice behaviors using an enthalpy method. The film model captures possible operating modes that involve frost formation and melting, as well as retained water/ice refreezing and melting under cyclic frosting and defrosting operations. The advantage of using the enthalpy method to model these phase change behaviors is that it features a uniform model structure with switched source terms for various operating modes. A switching algorithm based on the Fuzzy modeling approach is proposed to avoid discontinuities under mode switches. The developed film volume model is incorporated into a discretized outdoor flat-tube heat exchanger model, which is then coupled to other component models of an automotive heat pump system. The system model is simulated to investigate system responses under cyclic heating and reverse-cycle defrosting operations from a startup. Simulation results indicate that the developed model is capable of capturing the complicated cyclic transients of the refrigerant, air, frost, and retained water/ice in cases of insufficient defrost cycles. The proposed modeling approach can be extremely useful for improved defrost control designs and evaluations of air-source heat pumps as well as low-temperature refrigeration applications. Future research efforts can focus on experimentally validating the model in scenarios of retained water/ice presence and utilizing the model to investigate the overall impact of retained water refreezing and melting on system performance over multiple defrost cycles to aid performance improvements."
    },
    {
        "text": "In this study, we propose a real-time volumetric free-hand ultrasound imaging system, which can be built based on multiple types of ultrasound devices with a simple assembly process. This system is expected to be applied to clinical scenarios where equipment needs to be brought out for outdoor medical checkups. The system uses the optical localization method previously proposed by our research group, which can spatially localize the ultrasound probes with high accuracy in hospitals and other environments with strong electromagnetic interference. Moreover, we improved the recognition process of optical markers, together with parallel computing techniques, to achieve data acquisition and real-time visualization. This not only helps medical practitioners diagnose conditions promptly but also provides them with real-time 3D ultrasound images to guide them during surgeries. Successful 3D ultrasound imaging of the spine in different populations confirms the feasibility of the system for real-time 3D ultrasound imaging of large-sized organs. We hope to further apply it to the ultrasound imaging of other large-sized organs in clinical settings."
    },
    {
        "text": "The work here provides a new perspective on the limits of temporal resolution that is practically achievable in dynamic CT imaging. Typically, the gain in temporal resolution is quantified by the reduction in projections required per time frame to accurately reconstruct a 3D sample representation. This approach relates to the spatial complexity of the structure, often ignoring the temporal redundancies in longer dynamic CT acquisitions. This study demonstrates that the temporal resolution of the reconstruction can be improved depending on the complexity of the dynamics. It is, to a small extent, influenced by the interplay between the CT acquisition angle and the propagation direction of dynamic structures undergoing local changes in the attenuation coefficient. Previously, the rotation speed of the experiment needed to match the temporal resolution of single events in frame-based reconstruction. Now, it is more related to the time between subsequent events at the same location, relaxing the need for rapid rotations and shifting focus to global object states rather than individual local events. Results from experimental and synthetic datasets show that this reconstruction technique can accurately describe a variety of dynamic CT cases. This capability is essential for experimentally quantifying phenomena that can lead to more efficient and durable processes and materials in industry and academia."
    },
    {
        "text": "This paper highlights a fragility of certain large networked systems due to the localization of Laplacian eigenvectors. The fragility demonstrated in the examples may have significant implications for systems whose dynamics are modeled similarly to equation (4), such as those of AC transmission networks or oscillator networks more generally. A thorough investigation of such implications is the subject of further research. In this paper, we have not addressed the issue of which graph features cause eigenvector localization. This remains a topic for future research and is beyond the scope of the current paper. One conjecture arises from observations in Figure 1a, where we highlight the indices of localized nodes in red and delocalized nodes in blue. The latter corresponds to regions of degree homogeneity, while the former are in regions of degree heterogeneity, which might play a role as one cause of localization. Clearly, much more work is needed to uncover the causes and implications of this newly-discovered fragility phenomenon in networks."
    },
    {
        "text": "This paper presents the first implementation of an OTS (Optimal Transmission Switching) and BS (Bus Splitting) optimization model for AC/DC grids, in which switching actions can be performed on both the AC and DC grid parts, either jointly or separately. Results from test cases of varying sizes show that these actions lead to reduced generation costs compared to the original grid topology. While the benefits are test case-dependent, the findings confirm the effectiveness and potential of the proposed methodology. As future renewable-dominated AC/DC grids may increasingly face congestion issues, resorting to topological actions ensures more effective network utilization."
    },
    {
        "text": "In this work, we have obtained an approximation of the CDF of the distance to the closest visible BS in urban scenarios. By modeling the blockages as parallel line segments, we have provided bounds for this CDF under both independent and correlated blockages, along with the ergodic capacity. Additionally, we presented a method to account for the height of obstacles and compared the synthetic system model with a real layout of buildings. Future work could extend the analytical derivation to more sophisticated building models beyond those based on line segments, aiming to represent reality more accurately. Furthermore, the rate analysis could incorporate effects caused by human blocking and atmospheric conditions. Finally, heterogeneous networks with different tiers of BSs could also be considered."
    },
    {
        "text": "In this paper, we propose a two-stage adaptive robust optimization model for the J-UMUC strategy under the uncertainties of wind power and load demand. The first stage focuses on the J-UMUC schedule, while the second stage addresses economic dispatch. To solve this model, we propose an i-C&CG algorithm nested with an OA approach. Results from various cases demonstrate that the proposed method yields a more effective scheduling strategy under source-load uncertainty, showcasing significant cost-effectiveness and adaptability. Furthermore, we validate the efficiency advantage of the i-C&CG algorithm through parameter settings and comparisons with the standard C&CG method."
    },
    {
        "text": "This paper proposed a resilience-focused energy management model for grid-connected residential apartment complexes equipped with PV systems, battery storage, and EV charging stations. By addressing PV power uncertainty through a distributionally robust chance-constrained approach using the Wasserstein metric, the model ensured reliable and cost-effective operation. Real-world data, including PV generation, electric loads, and pricing structures for an existing apartment complex in Orlando, was used to validate the approach. The results demonstrated that integrating PV and battery systems reduced operational costs, lowered emissions, and supported EV adoption while offering a profitable business model for property owners. These findings highlight a practical and sustainable framework for advancing clean energy use in residential complexes. Future work will focus on incorporating additional uncertain parameters such as electric load and EV charging behavior. Machine learning-based optimization methods will also be explored to enhance the model’s scalability and decision-making under uncertainty."
    },
    {
        "text": "Enhancing JAIPW robustness through more flexible modeling of the selection model, as suggested by Salvatore et al. (2024), is a potential area of extension. Wu (2022) proposed a non-parametric extension of the PL method using kernel smoothing estimators. However, implementing such estimators poses significant challenges in cases involving multi-dimensional selection variables. Adapting the density ratio function for estimating selection weights in data missing at random (MAR) contexts, as suggested by Wang and Kim (2021), is another potential direction. For scenarios where individual-level data cannot be shared across cohorts, federated learning offers a privacy-preserving solution. This approach, which develops models using decentralized data without exchanging individual data points, can address selection bias while respecting privacy constraints. Research by Luo and Song (2020) provides a foundation for federated learning methodologies. Future work will focus on adapting these principles to mitigate selection bias in multi-center studies where data sharing is restricted. Finally, data from EHRs is subject to numerous biases beyond selection bias, such as misclassification, missing data, clinically informative patient encounter processes, confounding, lack of consistent data harmonization across cohorts, and true heterogeneity of the studied populations. Developing robust strategies to simultaneously address this confluence of biases remains a major challenge."
    },
    {
        "text": "The forecast horizon can be defined and applied to practically any ecological forecast, as demonstrated through both theoretical and applied cases. Their empirical computation, based on scoring functions, always requires a reference—be it observations or model simulations. Choices of the scoring function, the reference, and the error tolerance lead to different types of forecast horizons (actual, relative, potential). We suggest that ecological and environmental forecasts should be accompanied by computations of forecast horizons to facilitate interpretation and communication. Future studies will need to evaluate which choices should be recommended; for now, any of these combinations can be considered complementary information to the forecast itself. In summary, the demonstrated concepts may best be simultaneously applied and extended to establish a field of ecological predictability analysis. We outlined directions for further research and supported this with specific case studies. It could be impactful to demonstrate the relevance of forecast horizons using more recent examples with extensive data or more complex ecological systems, as discussed in [79, 16]. However, our case studies represent relevant scenarios, featuring models used in operational forecasting (Emulator), models used for decision making (iLand) in fields where predictive ecology is already practiced successfully [80], and models used to explore hypotheses qualitatively and theoretically (Ricker)."
    },
    {
        "text": "The generalized spatial autoregressive (GSAR) model presented in this paper offers an innovative extension to standard SAR models by incorporating the spatial dependence structure into the linear predictor for all variables whose distribution is within the exponential family. This approach enables capturing the interactions between spatial units for non-normal variables based on the maximization of quasi-likelihood and generalized estimating equations (GEE). It guarantees consistency and efficiency, improving upon the Poisson, logistic, and probit models already published and widely used in practice. The theoretical formulation is reinforced with the derivation of the asymptotic properties of the estimators, providing a robust framework for practical implementation. The algorithm has been developed in the R software, paving the way for its adoption as a standard tool in the analysis of complex spatial data. The proposed GSAR model proves to be more effective than other similar techniques for modeling data with spatial structure and non-normal distributions such as Poisson and Binomial. Specifically, in the case of the Poisson distribution, the proposed model captures the spatial parameter p more accurately, showing lower bias and variance compared to alternatives like GLM and homoscedastic normal models (SAR and SARAR). "
    },
    {
        "text": "This study presents a novel application of the Stochastic Search Variable Selection (SSVS) methodology to Generalized Linear Mixed Models (GLMMs), filling a gap in the existing literature. We show that results from previous work, particularly Yang et al. (2020), can be extended directly to GLMMs with only a modification of the likelihood. The proposed methodology addresses the challenges posed by high dimensionality and complex interactions commonly encountered when fitting GLMMs. The results indicate that the approach effectively removes small coefficients while retaining those critical to explaining the underlying relationships in the data. Simulation studies demonstrated the efficacy of the proposed method under various scenarios, including settings with both strong and weak signal-to-noise ratios. The results highlighted the SSVS approach’s ability to reliably identify true fixed and random effects, even in challenging conditions with small effect sizes or increased dimensionality. Furthermore, we found that the choice of hyperparameters did not significantly influence the performance of the model or the selection of coefficients, making the method easier and more robust for practical use. We also applied the proposed methodology to a real ecological data set with reasonable success. While the approach was somewhat unsatisfactory in selecting a unique set of fixed effects, it was able to identify a substantially simpler model for the random effects. Notably, this simpler model outperformed the full model in some posterior predictive checks, indicating its potential for practical applications."
    },
    {
        "text": "There is a natural concern that a theoretical result proved only for simple distributions (such as Gaussians) may not extend to more realistic problems. This skepticism is particularly valid for 'positive' results, such as demonstrating that an estimator converges quickly—a method might exploit specific properties of the simple distribution. However, the main result in this paper is 'negative'—it demonstrates that an estimator is slow for some class of simple distributions. We believe it would be quite surprising if generic methods, such as Stochastic Gradient Langevin Dynamics (SGLD), failed only on a small class of simple examples. Moreover, we are not aware of any empirical evidence suggesting that this phenomenon occurs."
    },
    {
        "text": "Bandit algorithms have gained significant attention and widespread applications across various fields. Accurate uncertainty quantification remains crucial for addressing the exploration-exploitation tradeoff inherent in these algorithms. In this paper, we reviewed two of the most commonly used methods: the Upper Confidence Bound (UCB) and Thompson Sampling (TS) approaches. Recent advancements, such as the multiplier bootstrap method [133] and perturbation-based methods [134,135], have shown promise in effectively managing the exploration-exploitation tradeoff in both multi-armed and linear bandit settings. These novel approaches are becoming increasingly relevant as researchers explore new scenarios. Structured and unstructured bandit problems represent two main categories within Multi-Armed Bandit (MAB) frameworks, each suited to specific applications and needs. As outlined in Table 2, these two types offer distinct advantages and limitations. Unstructured bandits are simple in design, easy to implement, and intuitive, making them ideal for straightforward applications. However, they may lack the efficiency of structured bandits, which leverage additional information to enhance decision-making, particularly in data-rich environments. Structured bandits, on the other hand, can quickly adapt to complex scenarios but may suffer from over- or under-exploration when incorrectly specified."
    },
    {
        "text": "In this work, we have described how IntraLayer’s infrastructure can be pivotal in executing a vision for a Platform of Digital Finance Platforms, utilizing algorithmically refined smart contracts acting as reliable decentralized fiduciaries. This model is essential for a world that is gradually—and then likely to suddenly—transitioning to a fully decentralized, efficient, and programmatic value exchange system. In the objectives section, we articulated the essence of our goals through specific optimization statements. The decision variables in these statements represent either system design parameters describing different services of the system or variables determining the allocation of economic resources within the network. While these problems are not solved in this work, they are presented with the singular objective of communicating IntraLayer’s overarching vision, which encapsulates its protocols and infrastructural components."
    },
    {
        "text": "The Schwartz-Smith two-factor model and its extensions have been widely used to estimate commodity futures for over two decades. However, these models are limited by their focus on local factors—those specific to individual markets—without accounting for interdependencies between different markets. In this paper, we propose a novel two-factor functional regression model that extends the Schwartz-Smith framework by incorporating interdependencies between the commodity futures market and the bond yields market. Additionally, kernel principal component analysis (kPCA) is applied to transform the functional regression problem into a finite-dimensional estimation problem. The latent short-term and long-term factors, along with the unknown parameters, are estimated jointly using the Kalman filter. In a comprehensive empirical analysis of WTI crude oil futures, we use US Treasury yields as the functional predictor to explore the relationship between these two markets. The results demonstrate that the proposed functional regression model provides more accurate futures price estimates than the Schwartz-Smith model, particularly for short-term contracts. Moreover, under normal economic conditions—when short-term Treasury yields are lower than long-term yields—the original Schwartz-Smith model shrinks after accounting for the effects of Treasury yields, with yields contributing more to long-term futures than to short-term futures. In contrast, during an economic recession, indicated by short-term Treasury yields exceeding long-term yields, the Schwartz-Smith model expands, with Treasury yields having a greater influence on short-term futures than on long-term futures."
    },
    {
        "text": "To alleviate the confusion caused by the numerous FinTech definitions from various studies, we developed a FinTech clustering framework with three interconnected perspectives: Technology, Model, and Stakeholder. The Technology perspective focuses on various FinTech-enabled technologies, while the Model perspective centers on FinTech models, ideas, innovations, applications, and businesses. The Stakeholder perspective encompasses various FinTech stakeholders, including FinTech start-ups, traditional financial institutions, technology developers, Tech Titans, government, and financial customers. These three perspectives are not isolated; FinTech technologies serve as the foundational enabler for the adoption and implementation of various FinTech models, which in turn create new business opportunities and impact different stakeholders. We illustrated the application of this clustering framework through an analysis of an Indian FinTech firm. This framework provides a more comprehensive and holistic view of FinTech and offers two practical implications for FinTech practitioners. First, FinTech entrepreneurs and practitioners must have a thorough understanding of contemporary FinTech technologies before recognizing the business opportunities these technologies bring (Kreuzer et al., 2022), as these opportunities lead to the formation of corresponding FinTech models that are essential for achieving sustainable business success. Second, analyzing FinTech models should not be conducted in isolation but must consider relevant stakeholders, as they play a critical role in determining the success of a FinTech model (Gray & Purdy, 2018)."
    },
    {
        "text": "In this article, the author’s prior work on isotropic correlation is applied to the emergent space of cryptocurrencies, focusing on a mainstream, retail-oriented set of assets available to clients of the popular brokerage Robinhood. The characteristic form of (N) is shown to align with expectations for a cross-section of returns described by a homoskedastic isotropic covariance structure, rather than that of a linear factor model, including a single-factor model. Evidence supports the validity of this data description for at least the past five years. Homoskedastic isotropic covariance is a particularly interesting structure as it appears to support a “market factor,” yet there is no common exogenous driving force for returns—similarities in returns occur frequently without a singular external cause. A critical feature of this structure is that the asymptotic contribution of residual returns to a “large” portfolio does not vanish, meaning portfolio returns can still be dominated by an individual asset. Given the high level of pairwise correlation, approximately 50%, a mean-variance optimal asset allocator should allocate based not on alpha over variance but rather on a “mostly centered” Z-score of the alpha, inversely weighted by volatility."
    },
    {
        "text": "Through an analysis of five years of EV registration data across a subset of counties, this paper examines the expected results from a fixed effect model alongside the unexpected findings of using NEVI funding as an instrumental variable for public charging stations on EV adoption. The results suggest that increasing public charging infrastructure may have limited significance in the broader context of this study. The policy implication derived from the NEVI estimates indicates that more effective policies may exist than incentivizing EV adoption through additional investment in public EV infrastructure. This does not imply that current trends in EV adoption are slowing; rather, the efficacy of public charging stations in driving further adoption might be overly optimistic. From a purely cost-benefit perspective, reducing the funding rate for the remainder of the program could be an economically prudent strategy. Although the estimates from this study may be influenced by endogeneity due to a potentially biased instrumental variable, the direction of these estimates and the statistical power of this study are sufficient to discourage increased NEVI-related investments unless new data suggests otherwise."
    },
    {
        "text": "Using Twitter’s quasi-exogenous geographic expansion following the 2007 SXSW festival and employing an instrumental variable approach, this study finds no significant impact of Twitter adoption on county-level suicide rates after accounting for various demographic, geographic, and socioeconomic factors. This absence of effect carries important implications for public policy, highlighting the need for a balanced approach that considers the diverse impacts of social media. It is crucial to recognize that not all social media platforms have uniform effects; while it cannot be claimed that social media universally has no impact on mental health outcomes, it is equally important to demonstrate that certain platforms—such as Twitter, at least prior to its 2022 acquisition by Elon Musk—show no measurable influence. Given the limitations in the external validity of this study, future research should expand on these findings by investigating other platforms and populations to better capture the heterogeneous effects of social media. Furthermore, understanding the evolving nature of platform algorithms and user interactions over time will be critical to assessing the broader implications of social media on mental health outcomes."
    },
    {
        "text": "In this work, we introduce a reward-and-penalty variable premium scheme for reinsurance policies. Unlike classical premium principles, which assign a fixed value based solely on the distribution of a random loss, the proposed scheme accounts for both the loss distribution and the realized loss amount, thereby introducing additional randomness for both the insurer and the reinsurer. We begin by discussing the properties of this variable premium scheme and comparing them with the desirable characteristics of classical premium principles. Subsequently, we establish an optimization framework for the insurer and characterize the optimal reinsurance under mild assumptions regarding the insurer’s risk measure. Specifically, when the insurer uses distortion risk measures or TVaR to quantify risk exposure, we further formulate optimal solutions. Additionally, a Bowley optimization problem is developed to capture the interaction between the insurer and the reinsurer. Through various numerical examples, we demonstrate that the reinsurer benefits from adopting the variable premium scheme, as it significantly reduces total risk exposure compared to the expected-value premium principle, which serves as a limiting case of the proposed scheme."
    },
    {
        "text": "We identify several key challenges in traditional Genetic Programming (GP) for alpha factor discovery, including the vast search space and the sparsity of effective solutions. Our findings show that GP performs better when focusing on promising regions rather than conducting random searches. To address this, we propose a new GP framework with carefully chosen initialization and structural constraints, forcing GP to focus its search on more promising areas. This approach is inspired by alpha-searching practices and aims to boost the efficiency of the discovery process. Analysis of 2020–2024 Chinese stock market data demonstrates that the proposed framework yields superior out-of-sample prediction results and higher portfolio returns compared to the benchmark. Several directions for further research remain. For instance, while we use a simple linear regression model to aggregate the alphas in this study, more complex machine learning or deep learning models could enhance the factor aggregation process. Designing specialized models for alpha synthesis represents a significant avenue for future work. Additionally, the computational cost of GP presents a major challenge. Despite its importance, there has been limited rigorous research on the time consumption of GP in the alpha selection process. Future research could investigate which steps are the most time-consuming and identify areas where efficiency can be improved. Addressing these challenges would be instrumental in promoting the broader application of GP-based methods in alpha selection."
    },
    {
        "text": "In this work, we introduced a novel method for estimating the score function within diffusion models to generate synthetic financial market scenarios. By leveraging Gauss-Hermite quadrature instead of conventional Monte Carlo estimation, our approach effectively captures the distribution of asset returns, particularly its tails, which are often characterized by sparse data points. This capability makes the method especially well-suited for applications in financial modeling. While this study serves as a proof of concept for the proposed algorithm, we employed a two-layer neural network for parameterization, which represents just one of many possible configurations. A detailed study of its application to portfolio optimization will be presented in a separate paper. Future research will focus on expanding this work along two primary directions, further refining its potential for practical and theoretical advancements in financial modeling."
    },
    {
        "text": "Global climate change has become one of the major challenges of this century, leading to various environmental issues such as global temperature rise, ocean acidification, sea level rise, shrinking ice sheets, and salinity intrusion. A significant portion of primary production relies on the survivability of aquatic ecosystems, but salinity intrusion disrupts the salinity balance in freshwater resources, ultimately impacting the distribution of flora in aquatic habitats. In this experiment, while Taro exhibited relatively less impact among the three plants due to its salt tolerance capacity, prolonged exposure to increased salinity proved toxic even for this hydrophyte. Histological observations revealed that the epidermis of Eichhornia crassipes and Enhydra fluctuans thickened, and their vascular bundles expanded as a defense mechanism against saline water intrusion. Despite these adaptations, the limited salt tolerance of hydrophytes makes them unable to survive under prolonged exposure, leading to cascading effects on ecosystems and humanity, which is ultimately responsible for initiating climate change. Therefore, urgent measures must be taken to mitigate global climate change to preserve the integrity of not only aquatic ecosystems but also terrestrial ecosystems, which remain vulnerable to the consequences of this ongoing crisis."
    },
    {
        "text": "In this article, we have examined the significance of a class of go-or-grow models, widely employed to describe various biological phenomena, particularly those related to cell dynamics and cancer progression. What began as a simple review of these models evolved into a mathematically involved exposition. Our primary objective has been to provide a comprehensive overview of the intriguing mathematical features that characterize this approach, while also identifying challenges and open problems that may inspire future mathematical investigations. Despite the biological relevance of these models and their extensive applications across various mathematical contexts, a systematic exploration of their mathematical properties and the associated analytical and numerical challenges has been notably absent. While we do not claim our analysis is exhaustive, we have aimed to delve into the diverse analytical results available in the literature, examining different mathematical assumptions related to the switching terms of the models and establishing clear connections between the classical FKPP equation and its go-or-grow counterparts."
    },
    {
        "text": "Here, we report the half-maximal response (EC50) and fold-activation by cortisol, aldosterone, and other corticosteroids for human MR rs5522, a haplotype containing valine at codon 180 instead of isoleucine found in the wild-type MR (Ile-180). MR rs5522 (Val-180) has been studied for its role in human brain functions related to coping with stress and depression. We compared the EC50 and fold-activation by corticosteroids of MR rs5522 and wild-type MR transfected into HEK293 cells using either the TAT3 promoter or the MMTV promoter. Parallel studies also investigated the binding of MR antagonists, spironolactone and progesterone, to MR rs5522. In HEK293 cells with the MMTV promoter, MR rs5522 exhibited a slightly higher EC50 compared to wild-type MR and a similar level of fold-activation across all corticosteroids. Conversely, in HEK293 cells with the TAT3 promoter, MR rs5522 demonstrated a higher EC50 (lower affinity) and higher fold-activation for cortisol compared to wild-type MR (Ile-180). Additionally, the EC50s of MR rs5522 for aldosterone and corticosterone were slightly lower, and fold-activation was higher relative to wild-type MR. Spironolactone and progesterone showed similar antagonist activity for MR rs5522 and wild-type MR in the presence of both MMTV and TAT3 promoters in HEK293 cells."
    },
    {
        "text": "Here, we introduced a statistical method for inferring leadership patterns from presence data collected at a single location. This method relies on randomizations of the input data to estimate p-values associated with identified leader-follower interactions. We tested the methodology against intuitive models of leading behavior and applied it to a real-world scenario involving presence data of reef manta rays at a cleaning station, recorded using passive acoustic telemetry. Our approach enabled the construction of a directed network from the manta detection data, representing leader-follower interactions within the tagged population, and facilitated an analysis of how covariates such as sex and size influence individuals' positions within the network. Results showed that female manta rays followed more males than expected, whereas males followed fewer females but with stronger associations. Regarding size, smaller mantas were found to follow other small mantas with weaker interaction strength, while other interactions within the network followed a pattern similar to that expected from randomized data. Additionally, we analyzed whether a follower was more likely to appear after the appearance of a leader at the cleaning station and confirmed that the observed associations indeed represented a leader-follower dynamic."
    },
    {
        "text": "Sequencing data often consist of a small number of samples and typically present challenges such as compositionality, sparsity, and spurious observations. Consequently, traditional network and statistical methods must be applied cautiously and with biological relevance in mind. To address these challenges, a methodology is proposed to infer and enhance a bio-network from 16S rRNA gene sequences. The proposed solutions are designed for filtering spurious species and interactions and can be applied either individually or in combination to any 16S microbiome dataset. Co-occurrence matrices were constructed for each experimental condition, and the inferred networks were enhanced with node and edge attributes. Additionally, the proposed filtering methods were demonstrated at various scales, including species (node) level and interaction (edge) level, to showcase their applicability and effectiveness."
    },
    {
        "text": "The gghic R package was developed to address the growing need for an intuitive and flexible tool to visualize genomic interaction data derived from Hi-C and related technologies. Leveraging the powerful ggplot2 framework, gghic integrates seamlessly into the R/Bioconductor ecosystem, providing a user-friendly yet highly customizable solution for generating publication-ready figures. The package introduces novel Stat and Geom objects to facilitate the visualization of triangular heatmaps, chromatin loops, TADs, and other genomic annotations, effectively filling a critical gap in the existing R packages available for genomic interaction analysis."
    },
    {
        "text": "Due to their remarkably robust dynamics, complex-balanced dynamical systems represent an important class of dynamical systems, particularly for their connection to the Global Attractor Conjecture [7, 5], which posits that these systems possess a globally attracting steady state within each invariant polyhedron. The set of rate constants in a network that generate complex-balanced systems form a variety known as the toric locus [44], with its codimension determined by the deficiency of the reaction network [5]. A generalization of the toric locus involves the set of rate constants that yield dynamics equivalent to those of complex-balanced systems. When these rate constants can take both positive and negative values, the set is referred to as the R-disguised toric locus; if restricted to positive values, it is called the disguised toric locus [20]. It has been shown [22] that both the disguised toric locus and the R-disguised toric locus are path-connected. Furthermore, a recent study [23] established a lower bound for the dimensions of these loci."
    },
    {
        "text": "More generally, our findings extend the analogy between protein evolution and disordered physical systems, reinforcing the idea that protein dynamics exhibit characteristics of complex and strongly correlated systems. However, when comparing these results with statistical physics models exhibiting glassy dynamics, we identified qualitative differences that highlight the unique constraints imposed by natural evolution on proteins. Future work should aim to further elucidate the connections between epistasis, evolvability, and environmental selection, which may provide deeper insights into evolutionary strategies across diverse biological systems. Additionally, we anticipate that many of the ideas presented in this work will soon be amenable to experimental testing, driven by advancements in the power and capabilities of in vitro evolution platforms."
    },
    {
        "text": "The rise of complex multicellular ecosystems during the Neoproterozoic era was preceded by a microbial Proterozoic biosphere, where productivity was likely dominated by microbial mats composed of bacteria, including oxygenic photosynthetic Cyanobacteria, anoxygenic phototrophs, and heterotrophs. In modern environments, analogous microbial mats are found in restricted habitats such as carbonate tidal flats and terrestrial hot springs. Here, we report metagenomic sequence data from an analog in the hot springs of Waikite Valley, Aotearoa New Zealand, where carbon-rich, slightly alkaline geothermal waters support diverse phototrophic microbial mats."
    },
    {
        "text": "Purely descriptive analysis methods are insufficient to capture the complexity of processes observed in cell-fate decision-making systems. To address this, we need comprehensive frameworks that integrate diverse and heterogeneous data into a unified model. Decision-making inherently involves a temporal component, but our ability to directly observe these processes has been limited, relying instead on computational methods to infer temporal dynamics. Here, we argue that CellMaps or mechanistic models of cellular behavior are indispensable for interpreting the vast amounts of heterogeneous data available. These models not only provide critical functional insights but are also key to identifying design principles underlying cellular behavior. Such principles would offer a robust framework for reasoning about cell-fate decision-making processes, particularly when comparing systems across the tree of life. As research progresses, integrating and interpreting the functional and conceptual drivers of cell-fate decision-making across species and experimental systems will become an increasingly important priority."
    },
    {
        "text": "On broadly Copernican grounds, we are entitled to default assume that behaviorally sophisticated extraterrestrial entities would be conscious. Otherwise, it would seem inexplicably and implausibly lucky for humans to possess consciousness while similarly sophisticated entities elsewhere would be mere shells devoid of it. However, this Copernican default assumption is canceled when it comes to behaviorally sophisticated entities explicitly designed to mimic superficial features associated with human consciousness, such as many current, near-future, and hypothetical robots. These considerations, formulated as the Copernican and Mimicry Arguments, jointly undermine an otherwise attractive parity principle, which suggests we should apply the same behavioral or cognitive tests to aliens and robots, attributing or denying consciousness similarly based on their performance. Instead of grounding speculations about alien and robot consciousness in metaphysical or scientific theories regarding the physical or functional bases of consciousness, our approach appeals directly to the epistemic principles of Copernican mediocrity and inference to the best explanation. This framework allows us to justify certain default assumptions about consciousness while maintaining substantial neutrality regarding specific metaphysical and scientific theories."
    },
    {
        "text": "In this article, we obtained and analyzed a class of solutions that generalizes the original Hayward black hole solution. This generalization was achieved by incorporating a cosmological constant and a fluid of strings surrounding the Hayward black hole. We derived the metrics representing this class of spacetimes and investigated their regularity or singularity by examining the behavior of the Kretschmann scalar and the geodesics. Notably, an interesting and surprising result emerged: the predictions based on the analysis of the Kretschmann scalar were confirmed by the analysis of the geodesics."
    },
    {
        "text": "Perturbations of the gravitational field of a charged dilaton black hole involve complex interactions between coupled electromagnetic and scalar fields [13, 14], making the calculation of grey-body factors a challenging problem. However, once the quasinormal modes for these black holes were computed [14, 16], the correspondence between quasinormal modes and grey-body factors developed in [5, 6] could be employed. Using this correspondence, we addressed a gap in the literature on this classical solution by computing the grey-body factors for black holes with string theory corrections. Our findings reveal that grey-body factors are significantly suppressed when charge, coupled to the dilaton field, is introduced. Furthermore, we observed that axial and polar perturbations are no longer iso-spectral and exhibit considerable differences. Another intriguing aspect of grey-body factors is their greater stability under small static spacetime deformations [67, 68], in contrast to the overtones of the quasinormal spectrum, which are highly sensitive to near-horizon deformations [69, 70]. Additionally, the profile of a gravitational wave in the high-frequency regime can be modeled using grey-body factors [71, 72], reflecting the correspondence identified in [5, 6]. Consequently, the grey-body factors derived in this work not only provide a tool for accurately calculating the intensity of Hawking radiation but may also be applicable to modeling gravitational waves around dilaton black holes."
    },
    {
        "text": "The Friedmann spacetimes in the limit p=0, with or without a cosmological constant, have been the accepted large-scale model for late-stage Big Bang Cosmology since Hubble's 1929 measurement of the expanding Universe. In modern Cosmology, the zero-pressure Friedmann model applies after the pressure of the Universe dropped precipitously to zero, approximately 10,000 years after the Big Bang, roughly an order of magnitude before the decoupling of radiation and matter that gave rise to the microwave background radiation [17]. The widely accepted ΛCDM standard model for the large-scale expansion of the Universe describes a critically expanding k=0 Friedmann spacetime, where dark energy is modeled by a positive cosmological constant. This cosmological constant is negligible compared to the energy density at early times but accounts for approximately 70% of the energy density of the Universe at present."
    },
    {
        "text": "The MIT bag model, despite its simplicity, has long been the preferred framework for studying quark matter in compact stars. Fundamental questions, such as the potential presence of deconfined matter in neutron star (NS) cores, have traditionally been explored without accounting for interactions among the system's constituents. Within the Standard Model of particle physics, Quantum Chromodynamics (QCD) is recognized as the fundamental theory governing the strong interactions between quarks, mediated by gluons. However, the existing weak-coupling equations of state (EoS) for cold quark matter require refinement within the QCD framework to more realistically depict quark matter cores in compact stars. This study adopts a simple yet parameterized EoS for cold quark matter that addresses the limitations of the bag model and other low-energy effective models by incorporating the properties of interacting quark matter (IQM). By leveraging perturbative QCD expansions and color superconductivity, we provide a framework that accounts for inter-quark effects arising from strong interactions, potentially uncovering novel physical phenomena under the extreme conditions of matter and gravity found in the interiors of compact stars."
    },
    {
        "text":"Galaxy appearances reveal the physics of how they formed and evolved. Machine learning models can now exploit galaxies' information-rich morphologies to predict physical properties directly from image cutouts. Learning the relationship between pixel-level features and galaxy properties is essential for building a physical understanding of galaxy evolution, but we are still unable to explicate the details of how deep neural networks represent image features. To address this lack of interpretability, we present a novel neural network architecture called a Sparse Feature Network (SFNet). SFNets produce interpretable features that can be linearly combined in order to estimate galaxy properties like optical emission line ratios or gas-phase metallicity. We find that SFNets do not sacrifice accuracy in order to gain interpretability, and that they perform comparably well to cutting-edge models on astronomical machine learning tasks. Our novel approach is valuable for finding physical patterns in large datasets and helping astronomers interpret machine learning results."
    },
    {
        "text":"The changing-look blazars (CLBs) are the blazars that their optical spectral lines at different epochs show a significant changes and present a clear transition between the standard FSRQ and BL Lac types. The changing-look phenomena in blazars are highly significant for enhancing our understanding of certain physical problems of active galactic nuclei (AGNs), such as the potential mechanism of the state transition in the accretion process of the supermassive black holes in the central engine of AGNs, the possible intrinsic variation of the jet, and the connection between the accretion disk and the jet. Currently, the CLBs reported in the literature are still rare astronomical objects. In our previous work, we found that there are 8 physical properties parameters of CLBs located between those of FSRQs and those of BL Lacs. In order to search more CLB candidates (CLBCs), we employed the mclust Gaussian Mixture Modelling clustering algorithm to perform clustering analysis for the 255 subsets of the 8 physical properties parameters with 2250 blazars from the 4FGL-DR3. We find that there are 29 subsets with 3 groups (corresponding to bl lacs, fsrqs, and CLBCs), in which there are 4 subsets with the adjusted Rand index greater then 0.610 (ARI > 0.610). The combined clustering results from 4 subsets report that there are 111 CLBCs that includes 44 CLBs reported in previous literature and 67 new CLBCs, where 11 CLBCs labeled as BL Lac and 56 CLBCs labeled as FSRQ in 4FGL catalog."
    },
    {
        "text":"Potential contamination from low/intermediate-redshift galaxies, such as objects with a prominent Balmer break, affects the photometric selection of high-redshift galaxies through identification of a Lyman break. Traditionally, contamination is estimated from spectroscopic follow-up and/or simulations. Here, we introduce a novel approach to estimating contamination for Lyman-break galaxy (LBG) samples based on measuring spatial correlation with the parent population of lower redshift interlopers. We propose two conceptual approaches applicable to different survey strategies: a single large contiguous field and a survey consisting of multiple independent lines of sight. For a large single field, we compute the cross-correlation function between galaxies at redshift z∼6 and intermediate-redshift galaxies at z∼1.3. We apply the method to the CANDELS GOODS-S and XDF surveys and compare the measurement with simulated mock observations, finding that the contamination level in both cases is not measurable and lies below 5.5% (at 90% confidence). For random-pointing multiple field surveys, we measure instead the number count correlation between high-redshift galaxies and interlopers, as a two-point correlation analysis is not generally feasible. We show an application to the LBG samples at redshift z∼8 and the possible interloper population at z∼2 in the Brightest of Reionizing Galaxies (BoRG) survey. By comparing the Pearson correlation coefficient with the result from Monte Carlo simulations, we estimate a contamination fraction of 62+13−39%, consistent with previous estimates in the literature. These results validate the proposed approach and demonstrate its utility as an independent check of contamination in photometrically selected samples of high-redshift galaxies."
    },
    {
        "text":"Primordial Black Holes (PBHs) can form from gravitational collapse of large overdensities in the early Universe, giving rise to rich phenomena in astrophysics and cosmology. We develop a novel, general, and accurate method based on theory of density contrast peaks to calculate the abundance of PBHs for a broad power spectrum of curvature perturbations with Gaussian statistics. By introducing a window function to account for relevant perturbation scales for PBHs of different masses, as well as a filter function circumventing overproduction of small PBHs, we find that previous studies might have dramatically overestimated the abundance of PBHs by up to (10) orders of magnitude."
    },
    {
        "text":"We investigate the driving mechanism of Alfvén wave-driven stellar winds from red giant stars, Arcturus (α Boo; K1.5 III) and Aldebaran (α Tau; K5 III), with nonideal MHD simulations. Since the atmosphere is not fully ionized, upward propagating Alfvénic waves excited by surface convection are affected by ambipolar diffusion. Our fiducial run with the nonideal MHD effect for α Boo gives time-averaged mass-loss rate, M˙=3.3×10−11M⊙/yr, which is more than one order of magnitude reduced from the result in the ideal MHD run and nicely explains the observational value. Magnetized hot bubbles with T≳106K are occasionally present simultaneously with cool gas with T∼ a few thousands K in the atmosphere because of the thermal instability triggered by radiative cooling; there coexist fully ionized plasma emitting soft X-rays and molecules absorbing/emitting infrared radiations. The inhomogeneity in the atmosphere also causes large temporal variations in the mass-loss rate within an individual magnetic flux tube. We also study the effect of magnetic field strength and metallicity, and find that the wind density, and accordingly the mass-loss rate, positively and sensitively depends on both of them through the ambipolar diffusion of Alfvénic this http URL nonideal MHD simulation for α Tau, which is slightly more evolved than α Boo and has weaker magnetic field, results in weaker wind with M˙=1.5×10−12M⊙/yr with the atmospheric temperature ≲105K throughout the simulation time. However, given the observations implying the presence of locally strong magnetic fields on the surface of α~Tau, we also conduct a simulation with a field strength twice as strong. This results in M˙=2.0×10−11M⊙/yr -- comparable to the observed value -- with transient magnetized hot bubbles."
    },
    {
        "text":"We study the asymmetric interaction of wave-like velocity perturbation with a coronal current sheet (CS) in the presence of resistivity, thermal conduction (TC) and radiative cooling (RC). We analyze the dynamics and energetics of CS in four cases, namely, (i) no energy loss, (ii) TC only, (iii) RC only and, (iv) TC+RC. Before fragmentation, thinning and elongation of the CS are found to be identical in all four cases and therefore independent of presence or absence of energy loss effects. Onset times, corresponding Lundquist numbers and aspect ratios suggest that TC advances the onset of fragmentation while RC has the opposite effect in comparison to absence of energy losses. Reconnection takes place at a higher rate in presence of TC and TC+RC in the tearing unstable CS. The speed of plasmoids are also found to be higher under the effect of TC and TC+RC. In presence of TC and TC+RC, average density becomes higher within the tearing unstable CS than in other two cases. As expected, estimated average temperature is increasing with highest and lowest rate in absence of energy losses and in presence of both TC and RC respectively. After the onset of fragmentation, the rate of decrement of average magnetic energy density and increment of average kinetic energy density becomes higher in presence of TC and TC+RC than in other two cases. Thus we conclude that presence of energy loss mechanisms critically influence the dynamics, energetics, and plasmoid formation within a reconnecting coronal CS."
    },
    {
        "text":"Luminous Blue Variables (LBVs) are enigmatic, evolved, massive stars. Their variability has been observed to be episodic with large eruptions, along with variations on time-scales of days to decades. We have extracted light curves of 37 LBVs from the first four years of the TESS mission. These light curves provide two years of photometric time-series for stars in the LMC, with several months of data for Galactic or SMC targets. We analyze the Fourier properties of the stellar light curves to determine their characteristic frequencies and red noise amplitudes, comparing them to mass-loss parameters through Hα strength, and in the case of the LMC stars, B−V color and luminosity as estimated by their apparent g-magnitudes. We confirm the absence of correlation between any of the Fourier parameters and stellar parameters, implying that there is no trend in how these stars vary as measured with these photometric data, which may point towards these stars being an extension to the supergiant α Cygni variables and not a unique class of object with regards to their short-term variations."
    },
    {
        "text":"Recently, a short-duration GRB with supernova association (GRB 200826A) and two long-duration GRBs with kilonova associations (GRB 211211A and GRB 230307A) have been detected, which demolished the hope for a tidy connection between GRB duration and their progenitor systems. Here I summarize various physical factors that can shape the duration of a GRB and propose that the duration of a GRB can be defined by four factors: progentor, central engine, emitter, and geometry. The progenitor-defined duration is only relevant when the central engine is powered by accretion and when the modifications by other factors are not important. The untidy situation of duration - progenitor mismatches suggests that other factors likely play important roles in defining GRB duration at least in some GRBs. In particular, a GRB may not be powered by accretion but rather by a millisecond magnetar at least for some GRBs. The complicated lightcurve of GRB 211211A suggests both progenitor- and engine-defined durations, which may require a new type of progenitor system involving a white dwarf - neutron star merger with a magnetar merger product. The single broad pulse lightcurve with well-behaved energy-dependent behavior of GRB 230307A suggests an emitter-defined long duration. The central engine timescale may be short enough to be accommodated within the framework of a standard binary neutron star merger. Its spiky lightcurve with fast variability as well as extended X-ray emission suggest the existence of mini-jets in the global dissipation region, powered by an underlying magnetar."
    },
    {
        "text":"The in-flight calibration and performance of the Solar Disk Imager (SDI), which is a pivotal instrument of the Lyman-alpha Solar Telescope (LST) onboard the Advanced Space-based Solar Observatory (ASO-S) mission, suggested a much lower spatial resolution than expected. In this paper, we developed the SDI point-spread function (PSF) and Image Bivariate Optimization Algorithm (SPIBOA) to improve the quality of SDI images. The bivariate optimization method smartly combines deep learning with optical system modeling. Despite the lack of information about the real image taken by SDI and the optical system function, this algorithm effectively estimates the PSF of the SDI imaging system directly from a large sample of observational data. We use the estimated PSF to conduct deconvolution correction to observed SDI images, and the resulting images show that the spatial resolution after correction has increased by a factor of more than three with respect to the observed ones. Meanwhile, our method also significantly reduces the inherent noise in the observed SDI images. The SPIBOA has now been successfully integrated into the routine SDI data processing, providing important support for the scientific studies based on the data. The development and application of SPIBOA also pave new ways to identify astronomical telescope systems and enhance observational image quality. Some essential factors and precautions in applying the SPIBOA method are also discussed."
    },
    {
        "text":"Observational constraints on small-scale primordial gravitational waves are considerably weaker than those on large scales. We focus on scenarios with significant primordial gravitational waves and curvature perturbations on small scales, studying the energy density spectrum of the second-order TSIGW. By leveraging current data from CMB, BAO, and PTA, combined with the SNR analysis of LISA, we can investigate how tensor-scalar induced gravitational waves affect observations on various scales, thus constraining the parameter space for primordial gravitational waves and curvature perturbations. The Bayes factor analysis suggests that TSIGW+PGW might be more likely to dominate current PTA observations compared to SMBHB."
    },
    {
        "text":"The concept of electromotive field appears in various applications in space and astrophysical plasmas. A review is given on the electromotive field highlighting our current understanding of the theoretical picture and the spacecraft observations in interplanetary space. The electromotive field is a key concept to successfully close the set of turbulent magnetohydrodynamic equations and also to construct a more complete picture of space plasma turbulence. Applications to astrophysical cases (Earth magnetosphere, heliospheric shocks, interstellar medium, and relativistic jets) are also briefly introduced, as well."
    },
    {
        "text":"We review observations of solar activity, geomagnetic variation, and auroral visibility for the extreme geomagnetic storm on 1872 February 4. The extreme storm (referred to here as the Chapman-Silverman storm) apparently originated from a complex active region of moderate area (approx 500 ) that was favorably situated near disk center (S19° E05°). There is circumstantial evidence for an eruption from this region at 9--10 UT on 1872 February 3, based on the location, complexity, and evolution of the region, and on reports of prominence activations, which yields a plausible transit time of approx29 hr to Earth. Magnetograms show that the storm began with a sudden commencement at approx14:27 UT and allow a minimum Dst estimate of £ -834 nT. Overhead aurorae were credibly reported at Jacobabad (British India) and Shanghai (China), both at 19°.9 in magnetic latitude (MLAT) and 24°. 2 in invariant latitude (ILAT). Auroral visibility was reported from 13 locations with MLAT below |20|° for the 1872 storm (ranging from |10°. 0|--|19°. 9| MLAT) versus one each for the 1859 storm (|17°. 3| MLAT) and the 1921 storm (|16.°2| MLAT). The auroral extension and conservative storm intensity indicate a magnetic storm of comparable strength to the extreme storms of 1859 September (25°.1 pm 0°.5 ILAT and -949 pm 31 nT) and 1921 May (27°.1 ILAT and -907 pm 132 nT), which places the 1872 storm among the three largest magnetic storms yet observed."
    },
    {
        "text":"Enhanced modeling of microlensing variations in light curves of strongly lensed quasars improves measurements of cosmological time delays, the Hubble Constant, and quasar structure. Traditional methods for modeling extra-galactic microlensing rely on computationally expensive magnification map generation. With large datasets expected from wide-field surveys like the Vera C. Rubin Legacy Survey of Space and Time, including thousands of lensed quasars and hundreds of multiply imaged supernovae, faster approaches become essential. We introduce a deep-learning model that is trained on pre-computed magnification maps covering the parameter space on a grid of k, g, and s. Our autoencoder creates a low-dimensional latent space representation of these maps, enabling efficient map generation. Quantifying the performance of magnification map generation from a low dimensional space is an essential step in the roadmap to develop neural network-based models that can replace traditional feed-forward simulation at much lower computational costs. We develop metrics to study various aspects of the autoencoder generated maps and show that the reconstruction is reliable. Even though we observe a mild loss of resolution in the generated maps, we find this effect to be smaller than the smoothing effect of convolving the original map with a source of a plausible size for its accretion disk in the red end of the optical spectrum and larger wavelengths and particularly one suitable for studying the Broad-Line Region of quasars. Used to generate large samples of on-demand magnification maps, our model can enable fast modeling of microlensing variability in lensed quasars and supernovae."
    },
    {
        "text":"The evolution of the orbits of bodies ejected from the Earth, Moon, Mercury and Mars was studied. At ejection velocities about 12-14 km/s, the fraction of bodies ejected from the Earth that fall back onto the Earth was about 0.15-0.25. The total number of bodies ejected from the Earth and delivered to the Earth and Venus probably did not differ much. The probability of collisions of bodies ejected from the Earth with the Moon moving in its present orbit was of the order of 0.01. Probabilities of collisions of bodies ejected from the Earth with Mercury were about 0.02-0.08 at ejection velocities greater than 11.3 km/s. The probabilities of collisions of bodies ejected from the Earth with Mars did not exceed 0.025. For the ejection of bodies from the present orbit of the Moon, probabilities of collisions of ejected bodies with planets were similar to those ejected from the Earth if we consider smaller ejection velocities from the Moon than from the Earth. The probability of a collision of a body ejected from Mars with Mars usually did not exceed 0.04 at an ejection velocity greater than 5.3 km/s. The fraction of bodies ejected from Mars and collided with Mercury was typically less than 0.08. Probabilities of collisions of bodies ejected from Mars with the Earth and Venus were about 0.1-0.2 (each) at an ejection velocity between 5.05 and 10 km/s. Most of bodies ejected from Mercury fall back onto Mercury. Probabilities of collisions of bodies ejected from Mercury with the Earth typically did not exceed 0.02 and 0.1 at an ejection velocity less than 8 km/s and 15 km/s, respectively. The fraction of bodies ejected from Mercury and collided with Venus was greater than that with the Earth typically by an order of magnitude. Probabilities of collisions of bodies with Venus were about 0.1-0.3 at a velocity of ejection from Mercury between 4.3 and 10 km/s."
    },
    {
        "text":"The newly accessible mid-infrared (MIR) window offered by the James Webb Space Telescope (JWST) for exoplanet imaging is expected to provide valuable information to characterize their atmospheres. In particular, coronagraphs on board the JWST Mid-InfraRed instrument (MIRI) are capable of imaging the coldest directly imaged giant planets at the wavelengths where they emit most of their flux. The MIRI coronagraphs have been specially designed to detect the NH3 absorption around 10.5 microns, which has been predicted by atmospheric models. We aim to assess the presence of NH3 while refining the atmospheric parameters of one of the coldest companions detected by directly imaging GJ 504 b. Its mass is still a matter of debate and depending on the host star age estimate, the companion could either be placed in the brown dwarf regime or in the young Jovian planet regime. We present an analysis of MIRI coronagraphic observations of the GJ 504 system. We took advantage of previous observations of reference stars to build a library of images and to perform a more efficient subtraction of the stellar diffraction pattern. We detected the presence of NH3 at 12.5 sigma in the atmosphere, in line with atmospheric model expectations for a planetary-mass object and observed in brown dwarfs within a similar temperature range. The best-fit model with Exo-REM provides updated values of its atmospheric parameters, yielding a temperature of Teff = 512 K and radius of R = 1.08 RJup. These observations demonstrate the capability of MIRI coronagraphs to detect NH3 and to provide the first MIR observations of one of the coldest directly imaged companions. Overall, NH3 is a key molecule for characterizing the atmospheres of cold planets, offering valuable insights into their surface gravity. These observations provide valuable information for spectroscopic observations planned with JWST."
    },
    {
        "text":"Patterned magnetic nanostructures are advanced materials characterized by their unique magnetic properties at the nanoscale, which are the result of tailored geometric configurations and compositional engineering. As interest in nanotechnology continues to grow exponentially, the exploration of patterned magnetic nanostructures turns into a vibrant and critical area of study for both industry professionals and academic researchers. Here, we review investigations of standing spin waves (collective spin precessions) in magnetic elements by the technique of ferromagnetic resonance (FMR). The presentation encompasses earlier studies of arrays of magnetic nanodisks and nanoringes by cavity-based FMR as well as more recent studies of individual nanodisks and 3D nanovolcanoes by a broadband FMR method which implies the use of a coplanar waveguide. Overall, the manuscript outlines the development of FMR studies along the two major lines: (i) downscaling from multiple to individual magnetic nanoelements and (ii) extending planar nanomagnets into the third dimension."
    },
    {
        "text":"We investigate the terahertz conductivity of conventional superconductors in Voigt and Faraday magneto-optical configurations. In the Voigt geometry, an ultrathin superconducting film is fully penetrated by the magnetic field which interacts with the spin, thus modifying the magnitudes of the optical gap and of the density of the condensate. We provide an alternative interpretation of the recent experiments showing the gapless conductivity of a Nb film measured by Lee et al. [1] which describes better their data for magnetic field above 1 T. In the Faraday geometry, we analyze the terahertz conductivity of three NbN films with varying thicknesses using the Maxwell-Garnett model, treating vortices as normal-state inclusions within a superconducting matrix. Moreover, we effectively account for ubiquitous pair-conserving and magnetic-field-dependent pair-breaking disorder scattering processes using the model of Herman and Hlubina [2]."
    },
    {
        "text":"Phonon polaritons are hybrid modes combining lattice dynamics and electromagnetic waves. Their behavior at long wavelengths is effectively described by Huang's equations. Here, we investigate phonon polaritons within 2D materials featuring twisted moiré structures. The interaction between electromagnetic waves and phonons of varying wavelengths gives rise to rich polaritons with moiré characteristics. We observe the polariton dividing into multiple branches, akin to coupled oscillators. Through numerical simulations based on realistic lattice models, we confirm the existence of these intriguing modes. A distinctive trait of moiré polar crystals is their spatially varying near-field response, offering robust signals for the experimental confirmation."
    },
    {
        "text":"A recently discovered family of Kagome lattice materials, has attracted great interest, especially in the debate over its dominant superconducting pairing symmetry. To explore this issue, we study the superconducting pairing behavior within the Kagome-Hubbard model through the constrained path Monte Carlo method. It is found that doping around the Dirac point generates a dominant next-nearest-neighbour-d pairing symmetry driven by on-site Coulomb interaction U. However, when considering the nearest-neighbor interaction V, it may induce nearest-neighbor-p pairing to become the preferred pairing symmetry. Our results provide useful information to identify the dominant superconducting pairing symmetry in family."
    },
    {
        "text":"Employing the state-of-the-art time-dependent variational principle algorithm, we compute the spectra of charge neutral excitations in the ν=1/2 fractional Chern insulator (FCI) on the Haldane honeycomb lattice model with hard-core bosons. The magnetoroton visualized from the dynamic density structure factor, acquire a minimum gap at finite momentum that can go soft with increasing interaction and give rise to a charge density wave (CDW) at the same wavevector. The sharper and more concentrated spectral weight of the roton mode is observed when approaching the FCI-CDW transition while the single-particle gap remains intact. We also resolve the graviton mode around Γ point of the Brillouin zone by measuring the dynamic quadrupolar density correlations. Interestingly, we not only reveal the graviton response is chiral for a certain FCI, but also show the different chiralities of the graviton of opposite-sign Hall conductance for the first time. Our results offer the clear spectral observations of magnetoroton and chiral graviton in a lattice model of FCI and will have relevance towards the on-going experiments of tunable FCIs in quantum moiré materials."
    },
    {
        "text":"A knowledge gap exists for flows and transport phenomena at the Angstrom scale when the Poisson Nernst Planck equation based on the concept of electrical double layer (EDL) fails. We discovered that streaming conductance becomes pressure dependent in Angstrom channels using latent track membranes. The streaming current emerges only when the applied pressure exceeds a threshold value, which is inconsistent with the existing knowledge as a constant. With increasing channel size, we found that the pressure dependent streaming conductance phenomenon weakens and vanishes into a constant streaming conductance regime when the mean channel radius exceeds 2 nm. The effective surface potential derived from the stream conductance that divides conduction anomalously increases as the channel narrows. We suspect the pressure dependent streaming current is due to the reinforced Coulomb interaction between counterions and deprotonated carboxyl groups at the surface, which is close to the ion channel but different from the electrified 2D materials. The streaming current emerged due to hydrodynamic friction when the counterions were released from the surface. We approximated the stochastic process of counterion dissociation by 1D Kramer escape theory framework and defined the Damkohler Number to describe the transition from nonlinear streaming conductance regime to linear regime as functions of applied pressure and channel radius and well explained the enhanced effective surface potential in confinement."
    },
    {
        "text":"The elucidation of the mechanism of SnV− formation in diamond is especially important as the SnV− color center has the potential to be a superior single-photon emitter when compared to the NV and to other Group IV color centers. The typical formation of the SnV involves placing Sn in diamond by ion implantation, but the formation of a charged SnV species requires an additional complication. This complication is related to the energy cost associated with electronic transitions within the host diamond. Effectively, producing the SnV− charge state using an electron obtained from a band edge of the host diamond is less energetically favorable than having the SnV− receive an electron from a neighboring donor dopant. Among donor dopants, substitutional N (NC) is always present in even the purest synthetic or natural diamond sample. The mechanism of electron donation by NC has been proposed by Collins for charging the NV in diamond and it has been used to interpret many experimental results. Therefore, in this paper we use DFT to explore the pathways for the formation of the SnV− charge state due to electron donation arising from the presence of NC in the host diamond. Explicitly, defect concentrations are calculated in equilibrium in each of the explored pathways to determine the yield of the SnV− throughout each of the pathways. The importance of our work is to suggest experimental ways of enhancing the yield of charged states like the SnV− in diamond for transformative applications in optoelectronics and quantum information."
    },
    {
        "text":"We consider macroscopic motion of 3He-A in global thermodynamic equilibrium within the context of the Zubarev statistical operator method. We formulate the corresponding effective theory in the language of the functional integral. The effective Lagrangian comprising macroscopic motion of fermionic excitations is calculated explicitly for the emergent relativistic fermions of the superfluid 3He-A-phase in the non-trivial bosonic background due to a space and time dependent matrix-valued vierbein featuring nonzero torsion as well as the Nieh-Yan anomaly. The matrix-valued vierbein formulation comprises an additional two dimensional internal spin space which may be replaced by one featuring a fermionic theory with a real valued vierbein, two Abelian gauge fields and a spin connection mixing the Dirac and internal spin spaces. As an application of the developed theory we consider macroscopic rotation around the axis of the pure integer mass vortices. The corresponding thermodynamical quantities of the normal component are analysed."
    },
    {
        "text":"Phase stabilization continues to be a critical issue in hafnium oxide (HfO2) due to the interdependence of various contributing factors. Using first-principles calculations, we analyze the effects of strain and doping on stabilizing the ferroelectric phase. We found that combining Y-doping, O-vacancy, and compressive biaxial strain, particularly in the (111) orientation, offers an optimal pathway for stabilizing the ferroelectric phase of HfO2. Analysis of structural coordination reveals how compressive strain affects phase competition. Crystallography analysis provides insights into the advantage of the (111) strain orientation compared to the (001) orientation. The impact of dopants is discussed in the context of these findings."
    },
    {
        "text":"The elastic tensor provides valuable insight into the mechanical behavior of a material with lattice strain, such as disordered binary alloys. Density functional theory (DFT)-based methods provide a powerful mechanism for computing and probing the microscopic features of elastic tensor-related properties. Here we present results for the rigid-ion and relaxed-ion elastic tensors computed using density functional perturbation theory (DFPT), for a comprehensive set of structural refractory body-centered cubic (BCC) binary alloys of molybdenum (Mo), niobium (Nb), tantalum (Ta), and tungsten (W). Intermediate quantities (force-response internal-strain tensor, Λ, and displacement-response internal-strain tensor, Γ) needed to compute the relaxed-ion elastic tensor are used here to map heterogeneity in elastic constants at each lattice site in a given alloy and associated relaxation fields. Derived polycrystalline aggregate properties -- the bulk modulus (B), shear modulus (G), Young's modulus (E), and Poisson's ratio (ν) and elastic anisotropy -- are reported as a function of composition for all binaries. Pugh's ratio (B/G) and the Cauchy pressure (C′) derived from computed elastic moduli are analyzed in order to evaluate the effect of alloying on the mechanical properties of the refractory BCC binary alloys. The computed mechanical properties data for the parent unary materials and binary alloys at systematically-varied Mo, Nb, Ta, and W compositions are in excellent agreement with available experimental data. These results, together with computed microscopic field data, establish a foundation for the principled design of compositionally-complex, high-temperature-structural refractory alloys with desired elastic properties."
    },
    {
        "text":"We define the absolute Wilson loop winding and prove that it bounds the quantum metric from below. This Wilson loop lower bound naturally reproduces the known Chern and Euler bounds of the quantum metric, and provides an explicit lower bound of the quantum metric due to the time-reversal protected Z2 index, answering a hitherto open question. In general, the Wilson loop lower bound can be applied to any other topological invariants characterized by Wilson loop winding, such as the particle-hole Z2 index. As physical consequences of the Z2 bound, we show that the time-reversal Z2 index bounds superfluid weight and optical conductivity from below, and bounds the direct gap of a band insulator from above."
    },
    {
        "text":"Quantum geometry, characterized by the quantum geometric tensor, is pivotal in diverse physical phenomena in quantum materials. In condensed matter systems, quantum geometry refers to the geoemtric properties of Bloch states in the Brillouin zone. This pedagogical review provides an accessible introduction to the concept of quantum geometry, emphasizing its extensive implications across multiple domains. Specifically, we discuss the role of quantum geometry in optical responses, Landau levels, and fractional Chern insulators, as well as its influence on superfluid weight, spin stiffness, exciton condensates, electron-phonon coupling, etc. By integrating these topics, we underscore the pervasive significance of quantum geometry in understanding emergent behaviors in quantum materials. Finally, we present an outlook on open questions and potential future directions, highlighting the need for continued exploration in this rapidly developing field."
    },
    {
        "text":"We present a multimethod investigation into the nature of the recently reported quantum spin liquid (QSL) phase in the spin-1/2 Heisenberg antiferromagnet on the Shastry-Sutherland lattice. A comprehensive projective symmetry group classification of fermionic mean-field Ansätze on this lattice yields 46 U(1) and 80 ℤ2 states. Motivated by density-matrix renormalization group (DMRG) calculations suggesting that the Shastry-Sutherland model and the square-lattice J1-J2 Heisenberg antiferromagnet putatively share the same QSL phase, we establish a mapping of our Ansätze to those of the square lattice. This enables us to identify the equivalent of the square-lattice QSL (Z2Azz13) in the Shastry-Sutherland system. Employing state-of-the-art variational Monte Carlo calculations with Gutzwiller-projected wavefunctions improved upon by Lanczos steps, we demonstrate the excellent agreement of energies and correlators between a gapless (Dirac) ℤ2 spin liquid -- characterized by only few parameters -- and approaches based on neural quantum states and DMRG. Furthermore, the real-space spin-spin correlations are shown to decay with the same power law as in the J1-J2 square lattice model, which also hosts a ℤ2 Dirac spin liquid. Finally, we apply the recently developed Keldysh formulation of the pseudo-fermion functional renormalization group to compute the dynamical spin structure factor; these correlations exhibit the features expected due to Dirac cones in the excitation spectrum, thus providing strong independent evidence for a Dirac QSL ground state. Our finding of a d-wave pairing ℤ2 Dirac QSL is consistent with the recently observed signatures of QSL behavior in Pr2Ga2BeO7 and outlines predictions for future experiments."
    },
    {
        "text":"Hybrid superconductor-semiconductor systems have received a great deal of attention in the last few years because of their potential for quantum engineering, including novel qubits and topological devices. The proximity effect, the process by which the semiconductor inherits superconducting correlations, is an essential physical mechanism of such hybrids. Recent experiments have demonstrated the proximity effect in hole-based semiconductors, but, in contrast to electrons, the precise mechanism by which the hole bands acquire superconducting correlations remains an open question. In addition, hole spins exhibit a complex strong spin-orbit interaction, with largely anisotropic responses to electric and magnetic fields, further motivating the importance of understanding the interplay between such effects and the proximity effect. In this work, we analyze this physics with focus on germanium-based two-dimensional gases. Specifically, we develop an effective theory supported by full numerics, allowing us to extract various analytical expressions and predict different types of superconducting correlations including non-standard forms of singlet and triplet pairing mechanisms with non-trivial momentum dependence; as well as different Zeeman and Rashba spin-orbit contributions. This, together with their precise dependence on electric and magnetic fields, allows us to make specific experimental predictions, including the emergence of f-type superconductivity, Bogoliubov Fermi surfaces, and gapless regimes caused by large in-plane magnetic fields."
    },
    {
        "text":"To reduce costs and delays related to developing new and effective drugs, there is a critical need for improved human liver tissue models. Here we describe an approach for 3D bioprinting functional human liver tissue models, in which we fabricate disc-shaped structures (discoids) 200m in thickness and 1-3 mm in diameter, embedded in a highly permeable support medium made from packed microgels. We demonstrate that the method is precise, accurate, and scalable; up to 100 tissues per hour can be manufactured with a variability and error in diameter of about 4%. Histologic and immunohistochemical evaluation of printed discs reveal self-organization, cell cohesion, and key liver marker expression. During the course of 3-4 weeks in culture, the tissues stably synthesize albumin and urea at high levels, outperforming spheroid tissue models. We find the tissues express more than 100 genes associated with molecular absorption, distribution, metabolism, and excretion (ADME) at levels within the range of human liver. The liver tissue models exhibit enzymatic formation of metabolites after exposure to multiple test compounds. Together, these results demonstrate the promise of 3D printed discoids for pharmacological and toxicological applications."
    },
    {
        "text":"We state and prove a birational realization of King's Conjecture for a category glued from the derived categories of all birational models in the GKZ fan of a toric variety. Our perspective extends ideas of Beilinson and Bondal to all semiprojective toric varieties. As a result, we obtain new and birationally-uniform applications to resolutions of the diagonal, categorical and noncommutative resolutions, monads, Frobenius generation, and window categories."
    },
    {
        "text":"We develop a theory of loops with involution. On this basis we define a Cayley-Dickson doubling on loops, and use it to investigate the lattice of varieties of loops with involution, focusing on properties that remain valid in the Cayley-Dickson double. Specializing to central-by-abelian loops with elementary abelian 2-group quotients, we find conditions under which one can characterize the automorphism groups of iterated Cayley-Dickson doubles. A key result is a corrected proof that for n>3, the automorphism group of the Cayley-Dickson loop Qn is GL3(𝔽2)×{±1}n−3."
    },
    {
        "text":"This paper is intended as a reference for some basic theory for dg categories and their bar complexes. Our modest goal is to carefully record the most important envelope operations can one perform on dg categories (in which one adjoins shifts, finite direct sums, or twists) and the inescapable sign rules that appear when combining these with opposite categories, tensor products, and the bar resolution. An appendix collects some theory of categorical idempotents that is useful when discussing bar complexes."
    },
    {
        "text":"Matroids give rise to several natural constructions of polytopes. Inspired by this, we examine polytopes that arise from the signed circuits of an oriented matroid. We give the dimensions of these polytopes arising from graphical oriented matroids and their duals. Moreover, we consider polytopes constructed from cocircuits of oriented matroids generated by the positive roots in any type A root system. We give an explicit description of their face structure and determine the Ehrhart series. We also study an action of the symmetric group on these polytopes, giving a full description the subpolytopes fixed by each permutation. These type A polytopes are graphic zonotopes, are polar duals of symmetric edge polytopes, and also make an appearance in Stapledon's paper introducing Equivariant Ehrhart Theory."
    },
    {
        "text":"We prove that the space of algebraic maps between two smooth projective varieties, under certain conditions, admit a configuration space model, thereby obtaining an algebro-geometric analogue of Bendersky-Gitler's result on topological function spaces. Our result is a natural higher dimensional counterpart of cite[Theorem 3]{Ban24}."
    },
    {
        "text":"This paper proposes risk-averse and risk-agnostic formulations to robust design in which solutions that satisfy the system requirements for a set of scenarios are pursued. These scenarios, which correspond to realizations of uncertain parameters or varying operating conditions, can be obtained either experimentally or synthetically. The proposed designs are made robust to variations in the training data by considering perturbed scenarios. This practice allows accounting for error and uncertainty in the measurements, thereby preventing data overfitting. Furthermore, we use relaxation to trade-off a lower optimal objective value against lesser robustness to uncertainty. This is attained by eliminating a given number of optimally chosen outliers from the dataset, and by allowing for the perturbed scenarios to violate the requirements with an acceptably small probability. For instance, we can pursue a riskier design that attains a lower objective value in exchange for a few scenarios violating the requirements, or we might seek a more conservative design that satisfies the requirements for as many perturbed scenarios as possible. The design of a flexible wing subject to aeroelastic constraints is used for illustration."
    },
    {
        "text":"Community detection, also known as graph partitioning, is a well-known NP-hard combinatorial optimization problem with applications in diverse fields such as complex network theory, transportation, and smart power grids. The problem's solution space grows drastically with the number of vertices and subgroups, making efficient algorithms crucial. In recent years, quantum computing has emerged as a promising approach to tackling NP-hard problems. This study explores the use of a quantum-inspired algorithm, Simulated Bifurcation (SB), for community detection. Modularity is employed as both the objective function and a metric to evaluate the solutions. The community detection problem is formulated as a Quadratic Unconstrained Binary Optimization (QUBO) problem, enabling seamless integration with the SB algorithm. Experimental results demonstrate that SB effectively identifies community structures in benchmark networks such as Zachary's Karate Club and the IEEE 33-bus system. Remarkably, SB achieved the highest modularity, matching the performance of Fujitsu's Digital Annealer, while surpassing results obtained from two quantum machines, D-Wave and IBM. These findings highlight the potential of Simulated Bifurcation as a powerful tool for solving community detection problems."
    },
    {
        "text":"We study the Einstein-Yang-Mills system in both the Lorenz and harmonic gauges, where the Yang-Mills fields are valued in any arbitrary Lie algebra , associated to any compact Lie group G. This gives a system of hyperbolic partial partial differential that does not satisfy the null condition and that has new complications that are not present for the Einstein vacuum equations nor for the Einstein-Maxwell system. We prove the exterior stability of the Minkowski space-time, ℝ1+3, governed by the fully coupled Einstein-Yang-Mills system in the Lorenz gauge, valued in any arbitrary Lie algebra , without any assumption of spherical symmetry. We start with an arbitrary sufficiently small initial data, defined in a suitable energy norm for the perturbations of the Yang-Mills potential and of the Minkowski space-time, and we show the well-posedness of the Cauchy development in the exterior, and we prove that this leads to solutions converging in the Lorenz gauge and in wave coordinates to the zero Yang-Mills fields and to the Minkowski space-time. This provides a first detailed proof of the exterior stability of Minkowski governed by the fully non-linear Einstein-Yang-Mills equations in the Lorenz gauge, by using a null frame decomposition that was first used by H. Lindblad and I. Rodnianski for the case of the Einstein vacuum equations. We note that in contrast to the much simpler case of the Einstein-Maxwell equations where one can omit the potential, in fact in the non-abelian case of the Einstein-Yang-Mills equations, the question of stability, or non-stability, is a purely gauge dependent statement and the partial differential equations depend on the gauge on the Yang-Mills potential that is needed to write up the equations."
    },
    {
        "text":"The ever-changing world of disease study heavily relies on mathematical models. They are key in finding and controlling infectious diseases. We aim to explore these mathematical tools used for studying disease spread in biology. The SEIR model holds our focus. It is a super important tool known for being flexible and useful. We look at the modified SEIR models' design and analysis. We dive right into vital parts like the equations that make the modified SEIR model work, setting parameter identities, and then checking its solutions' positivity and limits. The study begins with a detailed examination of the design and analysis of a modified SEIR model, demonstrating its angularity. We delve into the model's heart, dealing with critical issues such as the equations that drive the modified SEIR model, establishing parameter identities, and ensuring the positivity and boundlessness of its solutions. Basic Reproduction Number marks a significant milestone. We investigate the local stability, DFE, and EE. Global stability, a paramount consideration in understanding the long-term behaviors of the systems, is scrutinized by employing the Lyapunov stability theorem. The bifurcation analysis classifies and elucidates the fundamental concepts therein. One-dimensional bifurcation and forward and backward bifurcation analyses are intricately examined, providing a comprehensive understanding of the dynamical behavior and basic concepts. In summary, we offer a thorough description and analysis of the SEIR model but also lay the groundwork for advancing mathematical modeling in epidemiology. By bridging theoretical insights with practical implications, this study strives to empower researchers and policymakers with a deep understanding of infectious disease dynamics, thereby contributing to targeted public health strategies."
    },
    {
        "text":"In 1946, S. Ulam invented Monte Carlo method, which has since become the standard numerical technique for making statistical predictions for long-term behaviour of dynamical systems. We show that this, or in fact any other numerical approach can fail for the simplest non-linear discrete dynamical systems given by the logistic maps fa(x)=ax(1−x) of the unit interval. We show that there exist computable real parameters a∈(0,4) for which almost every orbit of fa has the same asymptotical statistical distribution in [0,1], but this limiting distribution is not Turing computable."
    },
    {
        "text":"Unmanned Aerial Vehicles (UAVs) are increasingly essential in various fields such as surveillance, reconnaissance, and telecommunications. This study aims to develop a learning algorithm for the path planning of UAV wireless communication relays, which can reduce storage requirements and accelerate Deep Reinforcement Learning (DRL) convergence. Assuming the system possesses terrain maps of the area and can estimate user locations using localization algorithms or direct GPS reporting, it can input these parameters into the learning algorithms to achieve optimized path planning performance. However, higher resolution terrain maps are necessary to extract topological information such as terrain height, object distances, and signal blockages. This requirement increases memory and storage demands on UAVs while also lengthening convergence times in DRL algorithms. Similarly, defining the telecommunication coverage map in UAV wireless communication relays using these terrain maps and user position estimations demands higher memory and storage utilization for the learning path planning algorithms. Our approach reduces path planning training time by applying a dimensionality reduction technique based on Principal Component Analysis (PCA), sample combination, Prioritized Experience Replay (PER), and the combination of Mean Squared Error (MSE) and Mean Absolute Error (MAE) loss calculations in the coverage map estimates, thereby enhancing a Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. The proposed solution reduces the convergence episodes needed for basic training by approximately four times compared to the traditional TD3."
    },
    {
        "text":"Mitigation of biases, such as language models' reliance on gender stereotypes, is a crucial endeavor required for the creation of reliable and useful language technology. The crucial aspect of debiasing is to ensure that the models preserve their versatile capabilities, including their ability to solve language tasks and equitably represent various genders. To address this issue, we introduce a streamlined Dual Dabiasing Algorithm through Model Adaptation (2DAMA). Novel Dual Debiasing enables robust reduction of stereotypical bias while preserving desired factual gender information encoded by language models. We show that 2DAMA effectively reduces gender bias in English and is one of the first approaches facilitating the mitigation of stereotypical tendencies in translation. The proposed method's key advantage is the preservation of factual gender cues, which are useful in a wide range of natural language processing tasks."
    },
    {
        "text":"Predictive modeling using structural magnetic resonance imaging (MRI) data is a prominent approach to study brain-aging. Machine learning algorithms and feature extraction methods have been employed to improve predictions and explore healthy and accelerated aging e.g. neurodegenerative and psychiatric disorders. The high-dimensional MRI data pose challenges to building generalizable and interpretable models as well as for data privacy. Common practices are resampling or averaging voxels within predefined parcels, which reduces anatomical specificity and biological interpretability as voxels within a region may differently relate to aging. Effectively, naive fusion by averaging can result in information loss and reduced accuracy. We present a conceptually novel two-level stacking ensemble (SE) approach. The first level comprises regional models for predicting individuals' age based on voxel-wise information, fused by a second-level model yielding final predictions. Eight data fusion scenarios were explored using as input Gray matter volume (GMV) estimates from four datasets covering the adult lifespan. Performance, measured using mean absolute error (MAE), R2, correlation and prediction bias, showed that SE outperformed the region-wise averages. The best performance was obtained when first-level regional predictions were obtained as out-of-sample predictions on the application site with second-level models trained on independent and site-specific data (MAE=4.75 vs baseline regional mean GMV MAE=5.68). Performance improved as more datasets were used for training. First-level predictions showed improved and more robust aging signal providing new biological insights and enhanced data privacy. Overall, the SE improves accuracy compared to the baseline while preserving or enhancing data privacy."
    },
    {
        "text":"Extreme multi-label learning (XML) is a task of assigning multiple labels from an extremely large set of labels to each data instance. Many current high-performance XML models are composed of a lot of hyperparameters, which complicates the tuning process. Additionally, the models themselves are adapted specifically to XML, which complicates their reimplementation. To remedy this problem, we propose a simple method based on ridge regression for XML. The proposed method not only has a closed-form solution but also is composed of a single hyperparameter. Since there are no precedents on applying ridge regression to XML, this paper verified the performance of the method by using various XML benchmark datasets. Furthermore, we enhanced the prediction of low-frequency labels in XML, which hold informative content. This prediction is essential yet challenging because of the limited amount of data. Here, we employed a simple frequency-based weighting. This approach greatly simplifies the process compared with existing techniques. Experimental results revealed that it can achieve levels of performance comparable to, or even exceeding, those of models with numerous hyperparameters. Additionally, we found that the frequency-based weighting significantly improved the predictive performance for low-frequency labels, while requiring almost no changes in implementation. "
    },
    {
        "text":"To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can be key in overcoming the communication challenges of distributing AI workloads onto more Lite-GPUs. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management."
    },
    {
        "text":"Dynamic predictive modeling using electronic health record (EHR) data has gained significant attention in recent years. The reliability and trustworthiness of such models depend heavily on the quality of the underlying data, which is largely determined by the stages preceding the model development: data extraction from EHR systems and data preparation. We list over forty challenges encountered during these stages and provide actionable recommendations for addressing them. These challenges are organized into four categories: cohort definition, outcome definition, feature engineering, and data cleaning. This list is designed to serve as a practical guide for data extraction engineers and researchers, supporting better practices and improving the quality and real-world applicability of dynamic prediction models in clinical settings."
    },
    {
        "text":"Efficient surgery room scheduling is essential for hospital efficiency, patient satisfaction, and resource utilization. This study addresses this challenge by introducing a novel concept of Random-Key Optimizer (RKO), rigorously tested on literature and new, real-world inspired instances. Our combinatorial optimization problem incorporates multi-room scheduling, equipment scheduling, and complex availability constraints for rooms, patients, and surgeons, facilitating rescheduling and enhancing operational flexibility. The RKO approach represents solutions as points in a continuous space, which are then mapped in the problem solution space via a deterministic function known as a decoder. The core idea is to operate metaheuristics and heuristics in the random-key space, unaware of the original solution space. We design the Biased Random-Key Genetic Algorithm with Q-Learning, Simulated Annealing, and Iterated Local Search for use within an RKO framework, employing a single decoder function. The proposed metaheuristics are complemented by lower-bound formulations, providing optimal gaps for evaluating the effectiveness of the heuristic results. Our results demonstrate significant lower and upper bounds improvements for the literature instances, notably proving one optimal result. Furthermore, the best-proposed metaheuristic efficiently generates schedules for the newly introduced instances, even in highly constrained scenarios. This research offers valuable insights and practical solutions for improving surgery scheduling processes, offering tangible benefits to hospitals by optimising resource allocation, reducing patient wait times, and enhancing overall operational efficiency."
    },
    {
        "text":"Automatic speech recognition (ASR) systems are well known to perform poorly on dysarthric speech. Previous works have addressed this by speaking rate modification to reduce the mismatch with typical speech. Unfortunately, these approaches rely on transcribed speech data to estimate speaking rates and phoneme durations, which might not be available for unseen speakers. Therefore, we combine unsupervised rhythm and voice conversion methods based on self-supervised speech representations to map dysarthric to typical speech. We evaluate the outputs with a large ASR model pre-trained on healthy speech without further fine-tuning and find that the proposed rhythm conversion especially improves performance for speakers of the Torgo corpus with more severe cases of dysarthria. Code and audio samples are available at this https URL."
    },
    {
        "text":"In epidemiology, traditional statistical methods such as logistic regression, linear regression, and other parametric models are commonly employed to investigate associations between predictors and health outcomes. However, non-parametric machine learning techniques, such as deep neural networks (DNNs), coupled with explainable AI (XAI) tools, offer new opportunities for this task. Despite their potential, these methods face challenges due to the limited availability of high-quality, high-quantity data in this field. To address these challenges, we introduce SEANN, a novel approach for informed DNNs that leverages a prevalent form of domain-specific knowledge: Pooled Effect Sizes (PES). PESs are commonly found in published Meta-Analysis studies, in different forms, and represent a quantitative form of a scientific consensus. By direct integration within the learning procedure using a custom loss, we experimentally demonstrate significant improvements in the generalizability of predictive performances and the scientific plausibility of extracted relationships compared to a domain-knowledge agnostic neural network in a scarce and noisy data setting. "
    },
    {
        "text":"Tokenization is a fundamental step in natural language processing, breaking text into units that computational models can process. While learned subword tokenizers have become the de-facto standard, they present challenges such as large vocabularies, limited adaptability to new domains or languages, and sensitivity to spelling errors and variations. To overcome these limitations, we investigate a hierarchical architecture for autoregressive language modelling that combines character-level and word-level processing. It employs a lightweight character-level encoder to convert character sequences into word embeddings, which are then processed by a word-level backbone model and decoded back into characters via a compact character-level decoder. This method retains the sequence compression benefits of word-level tokenization without relying on a rigid, predefined vocabulary. We demonstrate, at scales up to 7 billion parameters, that hierarchical transformers match the downstream task performance of subword-tokenizer-based models while exhibiting significantly greater robustness to input perturbations. Additionally, during continued pretraining on an out-of-domain language, our model trains almost twice as fast, achieves superior performance on the target language, and retains more of its previously learned knowledge. Hierarchical transformers pave the way for NLP systems that are more robust, flexible, and generalizable across languages and domains."
    },
    {
        "text":"Personalized learning represents a promising educational strategy within intelligent educational systems, aiming to enhance learners' practice efficiency. However, the discrepancy between offline metrics and online performance significantly impedes their progress. To address this challenge, we introduce Agent4Edu, a novel personalized learning simulator leveraging recent advancements in human intelligence through large language models (LLMs). Agent4Edu features LLM-powered generative agents equipped with learner profile, memory, and action modules tailored to personalized learning algorithms. The learner profiles are initialized using real-world response data, capturing practice styles and cognitive factors. Inspired by human psychology theory, the memory module records practice facts and high-level summaries, integrating reflection mechanisms. The action module supports various behaviors, including exercise understanding, analysis, and response generation. Each agent can interact with personalized learning algorithms, such as computerized adaptive testing, enabling a multifaceted evaluation and enhancement of customized services. Through a comprehensive assessment, we explore the strengths and weaknesses of Agent4Edu, emphasizing the consistency and discrepancies in responses between agents and human learners. The code, data, and appendix are publicly available at this https URL."
    },
    {
        "text":"This paper investigates the application of artificial intelligence (AI) in early-stage recruitment interviews in order to reduce inherent bias, specifically sentiment bias. Traditional interviewers are often subject to several biases, including interviewer bias, social desirability effects, and even confirmation bias. In turn, this leads to non-inclusive hiring practices, and a less diverse workforce. This study further analyzes various AI interventions that are present in the marketplace today such as multimodal platforms and interactive candidate assessment tools in order to gauge the current market usage of AI in early-stage recruitment. However, this paper aims to use a unique AI system that was developed to transcribe and analyze interview dynamics, which emphasize skill and knowledge over emotional sentiments. Results indicate that AI effectively minimizes sentiment-driven biases by 41.2%, suggesting its revolutionizing power in companies' recruitment processes for improved equity and efficiency."
    },
    {
        "text":"We explore an evolutionary search strategy for scaling inference time compute in Large Language Models. The proposed approach, Mind Evolution, uses a language model to generate, recombine and refine candidate responses. The proposed approach avoids the need to formalize the underlying inference problem whenever a solution evaluator is available. Controlling for inference cost, we find that Mind Evolution significantly outperforms other inference strategies such as Best-of-N and Sequential Revision in natural language planning tasks. In the TravelPlanner and Natural Plan benchmarks, Mind Evolution solves more than 98% of the problem instances using Gemini 1.5 Pro without the use of a formal solver."
    },
    {
        "text":"This paper is the second in a planned series aimed at envisioning a path to safe and beneficial artificial intelligence. Building on the conceptual insights of 'Common Sense Is All You Need,' we propose a more formal litmus test for common sense, adopting an axiomatic approach that combines minimal prior knowledge (MPK) constraints with diagonal or Godel-style arguments to create tasks beyond the agent's known concept set. We discuss how this approach applies to the Abstraction and Reasoning Corpus (ARC), acknowledging training/test data constraints, physical or virtual embodiment, and large language models (LLMs). We also integrate observations regarding emergent deceptive hallucinations, in which more capable AI systems may intentionally fabricate plausible yet misleading outputs to disguise knowledge gaps. The overarching theme is that scaling AI without ensuring common sense risks intensifying such deceptive tendencies, thereby undermining safety and trust. Aligning with the broader goal of developing beneficial AI without causing harm, our axiomatic litmus test not only diagnoses whether an AI can handle truly novel concepts but also provides a stepping stone toward an ethical, reliable foundation for future safe, beneficial, and aligned artificial intelligence."
    },
    {
        "text":"We introduce a prototyping testbed, GenSC-6G, developed to generate a comprehensive dataset that supports the integration of generative artificial intelligence (AI), quantum computing, and semantic communication for emerging sixth-generation (6G) applications. The GenSC-6G dataset is designed with noise-augmented synthetic data optimized for semantic decoding, classification, and localization tasks, significantly enhancing flexibility for diverse AI-driven communication applications. This adaptable prototype supports seamless modifications across baseline models, communication modules, and goal-oriented decoders. Case studies demonstrate its application in lightweight classification, semantic upsampling, and edge-based language inference under noise conditions. The GenSC-6G dataset serves as a scalable and robust resource for developing goal-oriented communication systems tailored to the growing demands of 6G networks."
    },
    {
        "text":"Early detection of forest fires is crucial to minimizing the environmental and socioeconomic damage they cause. Indeed, a fire's duration directly correlates with the difficulty and cost of extinguishing it. For instance, a fire burning for 1 minute might require 1 liter of water to extinguish, while a 2-minute fire could demand 100 liters, and a 10-minute fire might necessitate 1,000 liters. On the other hand, existing fire detection systems based on novel technologies (e.g., remote sensing, PTZ cameras, UAVs) are often expensive and require human intervention, making continuous monitoring of large areas impractical. To address this challenge, this work proposes a low-cost forest fire detection system that utilizes a central gateway device with computer vision capabilities to monitor a 360° field of view for smoke at long distances. A deep reinforcement learning agent enhances surveillance by dynamically controlling the camera's orientation, leveraging real-time sensor data (smoke levels, ambient temperature, and humidity) from distributed IoT devices. This approach enables automated wildfire monitoring across expansive areas while reducing false positives."
    },
    {
        "text":"Crash frequency modelling analyzes the impact of factors like traffic volume, road geometry, and environmental conditions on crash occurrences. Inaccurate predictions can distort our understanding of these factors, leading to misguided policies and wasted resources, which jeopardize traffic safety. A key challenge in crash frequency modelling is the prevalence of excessive zero observations, caused by underreporting, the low probability of crashes, and high data collection costs. These zero observations often reduce model accuracy and introduce bias, complicating safety decision making. While existing approaches, such as statistical methods, data aggregation, and resampling, attempt to address this issue, they either rely on restrictive assumptions or result in significant information loss, distorting crash data. To overcome these limitations, we propose a hybrid VAE-Diffusion neural network, designed to reduce zero observations and handle the complexities of multi-type tabular crash data (count, ordinal, nominal, and real-valued variables). We assess the synthetic data quality generated by this model through metrics like similarity, accuracy, diversity, and structural consistency, and compare its predictive performance against traditional statistical models. Our findings demonstrate that the hybrid VAE-Diffusion model outperforms baseline models across all metrics, offering a more effective approach to augmenting crash data and improving the accuracy of crash frequency predictions. This study highlights the potential of synthetic data to enhance traffic safety by improving crash frequency modelling and informing better policy decisions."
    },
    {
        "text":"Data imbalance is a common issue in analyzing and predicting sudden traffic events. Secondary crashes constitute only a small proportion of all crashes. These secondary crashes, triggered by primary crashes, significantly exacerbate traffic congestion and increase the severity of incidents. However, the severe imbalance of secondary crash data poses significant challenges for prediction models, affecting their generalization ability and prediction accuracy. Existing methods fail to fully address the complexity of traffic crash data, particularly the coexistence of dynamic and static features, and often struggle to effectively handle data samples of varying lengths. Furthermore, most current studies predict the occurrence probability and spatiotemporal distribution of secondary crashes separately, lacking an integrated solution. To address these challenges, this study proposes a hybrid model named VarFusiGAN-Transformer, aimed at improving the fidelity of secondary crash data generation and jointly predicting the occurrence and spatiotemporal distribution of secondary crashes. The VarFusiGAN-Transformer model employs Long Short-Term Memory (LSTM) networks to enhance the generation of multivariate long-time series data, incorporating a static data generator and an auxiliary discriminator to model the joint distribution of dynamic and static features. In addition, the model's prediction module achieves simultaneous prediction of both the occurrence and spatiotemporal distribution of secondary crashes. Compared to existing methods, the proposed model demonstrates superior performance in generating high-fidelity data and improving prediction accuracy."
    },
    {
        "text":"Leveraging the autonomous decision-making capabilities of large language models (LLMs) demonstrates superior performance in reasoning tasks. Despite the successes of iterative or recursive retrieval-augmented generation (RAG), they often are trapped in a single solution space when confronted with complex tasks. In this paper, we propose a novel thinking pattern in RAG which integrates system analysis with efficient reasoning actions, significantly activating intrinsic reasoning capabilities and expanding the solution space of specific tasks via Monte Carlo Tree Search (MCTS), dubbed AirRAG. Specifically, our approach designs five fundamental reasoning actions that are expanded to a wide tree-based reasoning spaces using MCTS. The extension also uses self-consistency verification to explore potential reasoning paths and implement inference scaling. In addition, computationally optimal strategies are used to apply more inference computation to key actions to achieve further performance improvements. Experimental results demonstrate the effectiveness of AirRAG through considerable performance gains over complex QA datasets. Furthermore, AirRAG is flexible and lightweight, making it easy to integrate with other advanced technologies."
    },
    {
        "text":"LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects (task definition, LLM profiling, and search procedures), making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. In this survey, we provide a comprehensive technical review that unifies task definitions and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For further details and ongoing updates, please refer to our GitHub repository: this https URL."
    },
    {
        "text":"In domains requiring intelligent agents to emulate plausible human-like behaviour, such as formative simulations, traditional techniques like behaviour trees encounter significant challenges. Large Language Models (LLMs), despite not always yielding optimal solutions, usually offer plausible and human-like responses to a given problem. In this paper, we exploit this capability and propose a novel architecture that integrates an LLM for decision-making with a classical automated planner that can generate sound plans for that decision. The combination aims to equip an agent with the ability to make decisions in various situations, even if they were not anticipated during the design phase."
    },
    {
        "text":"Increasingly many AI systems can plan and execute interactions in open-ended environments, such as making phone calls or buying online goods. As developers grow the space of tasks that such AI agents can accomplish, we will need tools both to unlock their benefits and manage their risks. Current tools are largely insufficient because they are not designed to shape how agents interact with existing institutions (e.g., legal and economic systems) or actors (e.g., digital service providers, humans, other AI agents). For example, alignment techniques by nature do not assure counterparties that some human will be held accountable when a user instructs an agent to perform an illegal action. To fill this gap, we propose the concept of agent infrastructure: technical systems and shared protocols external to agents that are designed to mediate and influence their interactions with and impacts on their environments. Agent infrastructure comprises both new tools and reconfigurations or extensions of existing tools. For example, to facilitate accountability, protocols that tie users to agents could build upon existing systems for user authentication, such as OpenID. Just as the Internet relies on infrastructure like HTTPS, we argue that agent infrastructure will be similarly indispensable to ecosystems of agents. We identify three functions for agent infrastructure: 1) attributing actions, properties, and other information to specific agents, their users, or other actors; 2) shaping agents' interactions; and 3) detecting and remedying harmful actions from agents. We propose infrastructure that could help achieve each function, explaining use cases, adoption, limitations, and open questions. Making progress on agent infrastructure can prepare society for the adoption of more advanced agents."
    },
    {
        "text":"The recent advancements in Generative Artificial intelligence (GenAI) technology have been transformative for the field of education. Large Language Models (LLMs) such as ChatGPT and Bard can be leveraged to automate boilerplate tasks, create content for personalised teaching, and handle repetitive tasks to allow more time for creative thinking. However, it is important to develop guidelines, policies, and assessment methods in the education sector to ensure the responsible integration of these tools. In this article, thematic analysis has been performed on seven essays obtained from professionals in the education sector to understand the advantages and pitfalls of using GenAI models such as ChatGPT and Bard in education. Exploratory Data Analysis (EDA) has been performed on the essays to extract further insights from the text. The study found several themes which highlight benefits and drawbacks of GenAI tools, as well as suggestions to overcome these limitations and ensure that students are using these tools in a responsible and ethical manner."
    },
    {
        "text":"With the advancement of information technology, the Social Internet of Things (SIoT) has fostered the integration of physical devices and social networks, deepening the study of complex interaction patterns. Text Attribute Graphs (TAGs) capture both topological structures and semantic attributes, enhancing the analysis of complex interactions within the SIoT. However, existing graph learning methods are typically designed for complete attributed graphs, and the common issue of missing attributes in Attribute Missing Graphs (AMGs) increases the difficulty of analysis tasks. To address this, we propose the Topology-Driven Attribute Recovery (TDAR) framework, which leverages topological data for AMG learning. TDAR introduces an improved pre-filling method for initial attribute recovery using native graph topology. Additionally, it dynamically adjusts propagation weights and incorporates homogeneity strategies within the embedding space to suit AMGs' unique topological structures, effectively reducing noise during information propagation. Extensive experiments on public datasets demonstrate that TDAR significantly outperforms state-of-the-art methods in attribute reconstruction and downstream tasks, offering a robust solution to the challenges posed by AMGs. The code is available at this https URL."
    },
    {
        "text":"The rapid advancement of digital technologies and recent global pandemic scenarios have led to a growing focus on how these technologies can enhance healthcare service delivery and workflow to address crises. Action plans that consolidate existing digital transformation programs are being reviewed to establish core infrastructure and foundations for sustainable healthcare solutions. Reforming health and social care to personalize home care, for example, can help avoid treatment in overcrowded acute hospital settings and improve the experiences and outcomes for both healthcare professionals and service users. In this information-intensive domain, addressing the interoperability challenge through standards-based roadmaps is crucial for enabling effective connections between health and social care services. This approach facilitates safe and trustworthy data workflows between different healthcare system providers. In this paper, we present a methodology for extracting, transforming, and loading data through a semi-automated process using a Common Semantic Standardized Data Model (CSSDM) to create personalized healthcare knowledge graph (KG). The CSSDM is grounded in the formal ontology of ISO 13940 ContSys and incorporates FHIR-based specifications to support structural attributes for generating KGs. We propose that the CSSDM facilitates data harmonization and linking, offering an alternative approach to interoperability. This approach promotes a novel form of collaboration between companies developing health information systems and cloud-enabled health services. Consequently, it provides multiple stakeholders with access to high-quality data and information sharing."
    },
    {
        "text":"Generative AI has had a profound impact on biomedicine and health, both in professional work and in education. Based on large language models (LLMs), generative AI has been found to perform as well as humans in simulated situations taking medical board exams, answering clinical questions, solving clinical cases, applying clinical reasoning, and summarizing information. Generative AI is also being used widely in education, performing well in academic courses and their assessments. This review summarizes the successes of LLMs and highlights some of their challenges in the context of education, most notably aspects that may undermines the acquisition of knowledge and skills for professional work. It then provides recommendations for best practices overcoming shortcomings for LLM use in education. Although there are challenges for use of generative AI in education, all students and faculty, in biomedicine and health and beyond, must have understanding and be competent in its use."
    },
    {
        "text":"Structural Equation Models (SEM) are the standard approach to representing causal dependencies between variables in causal models. In this paper we propose a new interpretation of SEMs when reasoning about Actual Causality, in which SEMs are viewed as mechanisms transforming the dynamics of exogenous variables into the dynamics of endogenous variables. This allows us to combine counterfactual causal reasoning with existing temporal logic formalisms, and to introduce a temporal logic, CPLTL, for causal reasoning about such structures. We show that the standard restriction to so-called \textit{recursive} models (with no cycles in the dependency graph) is not necessary in our approach, allowing us to reason about mutually dependent processes and feedback loops. Finally, we introduce new notions of model equivalence for temporal causal models, and show that CPLTL has an efficient model-checking procedure."
    },
    {
        "text":"The use of computational ontologies is well-established in the field of Medical Informatics. The topic of Social Determinants of Health (SDoH) has also received extensive attention. Work at the intersection of ontologies and SDoH has been published. However, a standardized framework for Social Determinants of Education (SDoEd) is lacking. In this paper, we are closing the gap by introducing an SDoEd ontology for creating a precise conceptualization of the interplay between life circumstances of students and their possible educational achievements. The ontology was developed utilizing suggestions from ChatGPT-3.5-010422 and validated using peer-reviewed research articles. The first version of developed ontology was evaluated by human experts in the field of education and validated using standard ontology evaluation software. This version of the SDoEd ontology contains 231 domain concepts, 10 object properties, and 24 data properties."
    },
    {
        "text":"Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publications, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. We proposed the concept of automated scholarly paper review (ASPR) in our previous paper. As the incorporation grows, it now enters the coexistence phase of ASPR and peer review, which is described in that paper. LLMs hold transformative potential for the full-scale implementation of ASPR, but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges associated with the development of LLMs for ASPR. We hope this survey can serve as an inspirational reference for the researchers and promote the progress of ASPR for its actual implementation."
    },
    {
        "text":"By retracing my scientific journey that began 20 years ago, I highlight in this thesis the need to consider the organization of the brain, which is certainly globally hierarchical, but also highly distributed and mixed in the neuronal response of its different areas (by focusing on the sensorimotor cortex-basal ganglia network). I also emphasize the importance of adopting behavioral paradigms that reflect as much as possible the characteristics of the scenarios encountered in the real life of animals. And finally, I mention the importance of 'disintegrating' the way data analysis is traditionally carried out, and of taking into account the dynamic nature of behavior, for example by favoring the study of behavioral variables and so-called 'latent' neuronal activities. I conclude this thesis by presenting the vision of my ideal laboratory in a perspective of 5 to 10 years from today. This laboratory would have two experimental devices, a first, classic, for carrying out tests in a constrained and therefore unnatural environment, the data of which would be compared to those from a second device called 'naturalistic' in which the animals could potentially express their entire behavioral repertoire. The tasks tested would be characterized by strong ecological principles, such as in those simulating the properties of foraging, and the data would make it possible to test hypotheses based on the progressive addition of ecological components in these tasks, all this in order to maintain control over the interpretability of these data which are complex by nature."
    },
    {
        "text":"The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are the most polymorphic in the human genome. It is generally accepted this polymorphism reflects a role in presenting pathogen-derived peptide to the adaptive immune system. Proposed mechanisms for the polymorphism such as negative frequency-dependent selection (NFDS) and heterozygote advantage (HA) focus on HLA alleles, not haplotypes. Here, we propose a model for the polymorphism in which infectious diseases impose independent density-dependent regulation on HLA haplotypes. More specifically, a complex pathogen environment drives extensive host polymorphism through a guild of HLA haplotypes that are specialised and show incomplete peptide recognition. Separation of haplotype guilds is maintained by limiting similarity. The outcome is a wide and stable range of haplotype densities at steady-state in which effective Fisher fitnesses are zero. Densities, and therefore frequencies, emerge theoretically as alternative measures of fitness. A catalogue of ranked frequencies is therefore one of ranked fitnesses. The model is supported by data from a range of sources including a Caucasian HLA dataset compiled by the US National Marrow Donor Program (NMDP). These provide evidence of positive selection on the top 350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5. High-fitness haplotypes drive the selection of 137 high-frequency alleles spread across the 5 HLA loci under consideration. These alleles demonstrate positive epistasis and pleiotropy in the formation of haplotypes. Allelic pleiotropy creates a network of highly inter-related HLA haplotypes that account for 97% of the census sample. We suggest this network has properties of a quasi-species and is itself under selection. We also suggest this is the origin of balancing selection in the HLA system."
    },
    {
        "text":"Epidemic spreading over populations networks has been an important subject of research for several decades, and especially during the Covid-19 pandemic. Most epidemic outbreaks are likely to create multiple mutations during their spreading over the population. In this paper, we study the evolution of a pathogen which can mutate continuously during the epidemic spreading. We consider pathogens whose mutating parameter is the mortality mean-time, and study the evolution of this parameter over the spreading process. We use analytical methods to compute the dynamic equation of the epidemic and the conditions for it to spread. We also use numerical simulations to study the pathogen flow in this case, and to understand the mutation phenomena. We show that the natural selection leads to less violent pathogens becoming predominant in the population. We discuss a wide range of network structures and show how different effects are manifested in each case."
    },
    {
        "text":"Muscles consume metabolic energy (ATP) to produce force. A mathematical model for energy expenditure can be useful in estimating real-time costs of movements or to predict energy optimal movements. Metabolic cost models developed so far have predominantly aimed at dynamic movement tasks, where mechanical work dominates. Further, while it is known that both force magnitude and rate of change of force (force rate) affect metabolic cost, it is not known how these terms interact, or if the force rate dependence can be a consequence of the force dependence. Here, we performed extensive human subject experiments, involving each subject over 5 hours of metabolic trials, which systematically changed the mean forces and forces rates so as to characterize a holistic relation for metabolic cost based on both force and force rate -- or analogously, torque and torque rate. Our experiments involved humans producing symmetric or asymmetric sinusoidal forces with different means, amplitudes, frequencies, and rise and fall periods. We showed that the metabolic cost can be well-approximated by a sum of power law functions of torque and torque rate. We found that the metabolic cost scales non-linearly with joint torque (with exponent = 1.36) and non-linearly with torque rate (with exponent = 2.5). Surprisingly, the data suggested that the cost was roughly four times higher for decreasing the torque than increasing, mirroring the analogous ratio between the cost of positive and negative work. Using these metabolic cost relations, we show that if the metabolic cost scales with particular exponents with muscle force and force rates, the same exponents will be observed in multi-joint tasks with multiple muscles. Our new metabolic cost model involving both force and force rate will potentially allow better predictions of energy optimal movements and thus inform wearable robot design and analysis."
    },
    {
        "text":"Humans use two qualitatively different gaits for locomotion, namely, walking and running -- usually using walking at lower speeds and running at higher speeds. Researchers have examined when humans switch between walking and running on treadmills and have noted hystereses in these gait transition speeds. Here, we consider an ecologically realistic overground locomotion task, one requiring traveling a given long distance (800 meters or 2400 meters) in a prescribed time duration. Unlike on a treadmill, this task allows the human to change speed or gait during the trial to reach the destination on time: this task is akin to traveling to an appointment at a particular time from your office to another office, arriving neither early or late. We find that gait transition is not sharp, but instead involves a 'gait transition regime' in which humans use a mixture of walking and running, using mostly walking atlower speeds and mostly running higher speeds -- supporting earlier results over short distances (120 m). The presence of this gradually changing walk-run mixture is predicted by energy optimality. We hypothesize that this energy optimal behavior in this realistic overground conditions accounts for the hysteretic behavior in treadmill experiments, apparently switching earlier than predicted by energy optimality."
    },
    {
        "text":"There is growing symbiosis between artificial and biological intelligence sciences: neural principles inspire new intelligent machines, which are in turn used to advance our theoretical understanding of the brain. To promote further collaboration between biological and artificial intelligence researchers, we introduce the 2025 edition of the Algonauts Project challenge: How the Human Brain Makes Sense of Multimodal Movies (this https URL). In collaboration with the Courtois Project on Neuronal Modelling (CNeuroMod), this edition aims to bring forth a new generation of brain encoding models that are multimodal and that generalize well beyond their training distribution, by training them on the largest dataset of fMRI responses to movie watching available to date. Open to all, the 2025 challenge provides transparent, directly comparable results through a public leaderboard that is updated automatically after each submission to facilitate rapid model assessment and guide development. The challenge will end with a session at the 2025 Cognitive Computational Neuroscience (CCN) conference that will feature winning models. We welcome researchers interested in collaborating with the Algonauts Project by contributing ideas and datasets for future challenges."
    },
    {
        "text":"C. elegans locomotion is composed of switches between forward and reversal states punctuated by turns. This locomotory capability is necessary for the nematode to move towards attractive stimuli, escape noxious chemicals, and explore its environment. Although experimentalists have identified a number of premotor neurons as drivers of forward and reverse motion, how these neurons work together to produce the behaviors observed remains to be understood. Towards a better understanding of C. elegans neurodynamics, we present in this paper a minimally parameterized, biophysical dynamical systems model of the premotor network. Our model consists of a recurrently connected collection of premotor neurons (the core group) driven by over a hundred sensory and interneurons that provide diverse feedforward inputs to the core group. It is data-driven in the sense that the choice of neurons in the core group follows experimental guidance, anatomical structures are dictated by the connectome, and physiological parameters are deduced from whole-brain imaging and voltage clamps data. When simulated with realistic input signals, our model produces premotor activity that closely resembles experimental data: from the seemingly random switching between forward and reversal behaviors to the synchronization of subnetworks to various higher-order statistics. We posit that different roles are played by gap junctions and synaptic connections in switching dynamics. The model correctly predicts behavior such as dwelling versus roaming as a result of the synaptic inputs received, and we demonstrate that it can be used to study how the activity level of certain individual neurons impacts behavior."
    },
    {
        "text":"The increasing importance of RNA as a prime player in biology can hardly be overstated. It is suspected that the functions of RNA are linked to their structures and dynamics. Many of the problems in RNA, such as folding and RNA-RNA interactions that drive phase separation even in the absence of proteins, require cations. Because experiments alone cannot directly reveal the dynamics of cation-RNA interactions, well calibrated theory and computations are needed to predict how ions control the behavior of RNA. In this review, we outline the development of coarse-grained models at different resolutions with application to phase separation in low complexity sequences. We also describe folding of ribozymes and riboswitches with a focus on the impact of monovalent and divalent cations. We outline major challenges that need to be overcome to simulate complex problems such as assembly of ribosomes."
    },
    {
        "text":"We extend the classical Susceptible-Infected-Recovered (SIR) model to a network-based framework where the degree distribution of nodes follows a Poisson distribution. This extension incorporates an additional parameter representing the mean node degree, allowing for the inclusion of heterogeneity in contact patterns. Using this enhanced model, we analyze epidemic data from the 2018-20 Ebola outbreak in the Democratic Republic of the Congo, employing a survival approach combined with the Hamiltonian Monte Carlo method. Our results suggest that network-based models can more effectively capture the heterogeneity of epidemic dynamics compared to traditional compartmental models, without introducing unduly overcomplicated compartmental framework."
    },
    {
        "text":"Antibodies are Y-shaped proteins that protect the host by binding to specific antigens, and their binding is mainly determined by the Complementary Determining Regions (CDRs) in the antibody. Despite the great progress made in CDR design, existing computational methods still encounter several challenges: 1) poor capability of modeling complex CDRs with long sequences due to insufficient contextual information; 2) conditioned on pre-given antigenic epitopes and their static interaction with the target antibody; 3) neglect of specificity during antibody optimization leads to non-specific antibodies. In this paper, we take into account a variety of node features, edge features, and edge relations to include more contextual and geometric information. We propose a novel Relation-Aware Antibody Design (RAAD) framework, which dynamically models antigen-antibody interactions for co-designing the sequences and structures of antigen-specific CDRs. Furthermore, we propose a new evaluation metric to better measure antibody specificity and develop a contrasting specificity-enhancing constraint to optimize the specificity of antibodies. Extensive experiments have demonstrated the superior capability of RAAD in terms of antibody modeling, generation, and optimization across different CDR types, sequence lengths, pre-training strategies, and input contexts."
    },
    {
        "text":"This paper examines strategic trading under incomplete information, where firms lack full knowledge of key aspects of their competitors' trading strategies such as target sizes and market impact models. We extend previous work on competitive trading equilibria by incorporating uncertainty through the framework of Bayesian games. This allows us to analyze scenarios where firms have diverse beliefs about market conditions and each other's strategies. We derive optimal trading strategies in this setting and demonstrate how uncertainty significantly impacts these strategies compared to the complete information case. Furthermore, we introduce a novel approach to model the presence of non-strategic traders, even when strategic firms disagree on their characteristics. Our analysis reveals the complex interplay of beliefs and strategic adjustments required in such an environment. Finally, we discuss limitations of the current model, including the reliance on linear market impact and the lack of dynamic strategy adjustments, outlining directions for future research."
    },
    {
        "text":"This paper proposes a model that enables permissionless and decentralized networks for complex computations. We explore the integration and optimize load balancing in an open, decentralized computational network. Our model leverages economic incentives and reputation-based mechanisms to dynamically allocate tasks between operators and coprocessors. This approach eliminates the need for specialized hardware or software, thereby reducing operational costs and complexities. We present a mathematical model that enhances restaking processes in blockchain systems by enabling operators to delegate complex tasks to coprocessors. The model's effectiveness is demonstrated through experimental simulations, showcasing its ability to optimize reward distribution, enhance security, and improve operational efficiency. Our approach facilitates a more flexible and scalable network through the use of economic commitments, adaptable dynamic rating models, and a coprocessor load incentivization system. Supported by experimental simulations, the model demonstrates its capability to optimize resource allocation, enhance system resilience, and reduce operational risks. This ensures significant improvements in both security and cost-efficiency for the blockchain ecosystem."
    },
    {
        "text":"The United States leads the world in the number of violent mass shootings that occur each year, and policy making on firearms remains polarized along party lines. Are legislators responsive to mass shootings? We estimate the latent positions of nearly 2,000 state legislators on gun policy from their roll-call voting records on firearm-related bills from 2011 to 2022. Employing a staggered difference-in-differences design, we find that mass shootings within or near a state legislator's district do not alter their voting behavior on firearm policy, on average, for members of both parties. Our estimated effects of mass shootings on treated legislators' support for restrictive gun policies (on a -1 to 1 scale) range from a 4.8% reduction among California Democrats and a 0.9% increase among California Republicans to, across six total states, a 5% (among Democrats) and 7.1% (among Republicans) increase, with 95% confidence intervals spanning opposite directions. We conclude that, on average, mass shootings fail to produce changes in a legislator's support (opposition) for restrictive (permissive) firearms bills. Our findings suggest that even the most heinous acts of mass violence -- that are squarely in the domain of events that state legislators might respond to -- fail to produce any measurable effects on legislators' positions on firearm-related policy."
    },
    {
        "text":"This paper studies the relationship between soft and hard paternalism by examining two kinds of restriction: a waiting period and a hard limit (cap) on risk-seeking behavior. Mandatory waiting periods have been instituted for medical procedures, gun purchases and other high-stakes decisions. Are these policies substitutes for hard restrictions, and are delayed decisions more respected? In an experiment, decision-makers are informed about an impending high-stakes decision. Treatments define when the decision is made: on the spot or after one day, and whether the initial decision can be revised. In a general population survey experiment, another class of subjects (Choice Architects) is granted the opportunity to make rules for decision-makers. Given a decision's temporal structure, Choice Architects can decide on a cap to the decision-maker's risk taking. In another treatment, Choice Architects can implement a mandatory waiting period in addition to the cap. This allows us to study the substitutional relationship between waiting periods and paternalistic action and the effect of deliberation on the autonomy afforded to the decision-maker. Our highly powered experiment reveals that exogenous deliberation has no effect on the cap. Moreover, endogenously prescribed waiting periods represent add-on restrictions that do not substitute for the cap. Choice Architects believe that, with time, the average decision-maker will take less risk and -- because of the distribution of Choice Architects' bliss points -- come closer to Choice Architects' subjective ideal choice. These findings highlight the complementarity of policy tools in targeting various parts of a distribution of decision-makers."
    },
    {
        "text":"Cryptocurrency investment is inherently difficult due to its shorter history compared to traditional assets, the need to integrate vast amounts of data from various modalities, and the requirement for complex reasoning. While deep learning approaches have been applied to address these challenges, their black-box nature raises concerns about trust and explainability. Recently, large language models (LLMs) have shown promise in financial applications due to their ability to understand multi-modal data and generate explainable decisions. However, single LLM faces limitations in complex, comprehensive tasks such as asset investment. These limitations are even more pronounced in cryptocurrency investment, where LLMs have less domain-specific knowledge in their training corpora. To overcome these challenges, we propose an explainable, multi-modal, multi-agent framework for cryptocurrency investment. Our framework uses specialized agents that collaborate within and across teams to handle subtasks such as data analysis, literature integration, and investment decision-making for the top 30 cryptocurrencies by market capitalization. The expert training module fine-tunes agents using multi-modal historical data and professional investment literature, while the multi-agent investment module employs real-time data to make informed cryptocurrency investment decisions. Unique intrateam and interteam collaboration mechanisms enhance prediction accuracy by adjusting final predictions based on confidence levels within agent teams and facilitating information sharing between teams. Empirical evaluation using data from November 2023 to September 2024 demonstrates that our framework outperforms single-agent models and market benchmarks in classification, asset pricing, portfolio, and explainability performance."
    },
    {
        "text":"The standard voting methods in the United States, plurality and ranked choice (or instant runoff) voting, are susceptible to significant voting failures. These flaws include Condorcet and majority failures as well as monotonicity and no-show paradoxes. We investigate alternative ranked choice voting systems using variations of the points-based Borda count which avoid monotonicity paradoxes. These variations are based on the way partial ballots are counted and on extending the values of the points assigned to each rank in the ballot. In particular, we demonstrate which voting failures are possible for each variation and then empirically study 421 U.S. ranked choice elections conducted from 2004 to 2023 to determine the frequency of voting failures when using five Borda variations. Our analysis demonstrates that the primary vulnerability of majority failures is rare or nonexistent depending on the variation. Other voting failures such as truncation or compromise failures occur more frequently compared to instant runoff voting as a trade-off for avoiding monotonicity paradoxes."
    },
    {
        "text":"This paper advances empirical demand analysis by integrating multimodal product representations derived from artificial intelligence (AI). Using a detailed dataset of toy cars on \textit{this http URL}, we combine text descriptions, images, and tabular covariates to represent each product using transformer-based embedding models. These embeddings capture nuanced attributes, such as quality, branding, and visual characteristics, that traditional methods often struggle to summarize. Moreover, we fine-tune these embeddings for causal inference tasks. We show that the resulting embeddings substantially improve the predictive accuracy of sales ranks and prices and that they lead to more credible causal estimates of price elasticity. Notably, we uncover strong heterogeneity in price elasticity driven by these product-specific features. Our findings illustrate that AI-driven representations can enrich and modernize empirical demand analysis. The insights generated may also prove valuable for applied causal inference more broadly."
    },
    {
        "text":"This paper analyzes and compares patterns of U.S. domestic rail freight volumes during, and after the disruptions caused by the 2007-2009 Great Recession and the COVID-19 pandemic in 2020. Trends in rail and intermodal shipment data are examined in conjunction with economic indicators, focusing on the extent of drop and recovery of freight volumes of various commodities and intermodal shipments, and the lead/lag time with respect to economic drivers. While impacts from and the rebound from the Great Recessions were slow to develop, COVID-19 produced both profound disruptions in the freight market and rapid rebound, with important variations across commodity types. Energy-related commodities (i.e., coal, petroleum, and fracking sand), dropped during the pandemic while demand for other commodities (i.e., grain products and lumber, and intermodal freight). rebounded rapidly and in some cases grew. Overall rail freight experienced a rapid rebound following the precipitous drop in traffic in March and April 2020, achieving a near-full recovery in five months. As the recovery proceeded through 2020, intermodal flow, containers moving by rail for their longest overland trips, rebounded strongly, some exceeding 2019 levels. In contrast, rail flows during the Great Recession changed slowly with the onset and recovery, extending over multiple years. Pandemic response reflected the impacts of quick shutdowns and a rapid shift in consumer purchasing patterns. Results for the pandemic illustrate the resilience of U.S. rail freight industry and the multifaceted role it plays in the overall logistics system. Amid a challenging logistical environment, freight rail kept goods moving when other methods of transport were constrained."
    },
    {
        "text":"We stress-test the European Defence Readiness in a Cold War 2.0 scenario analysis. Leveraging a general equilibrium multi-sector approach to the global economy, we simulate an abrupt decoupling from CRINK and evaluate impacts on defence readiness under changed future boundary conditions. Our results suggest that the defence industrial mobilisation, force mobility and sustained resilience readiness are largely off-track in view of the European Defence Readiness - defined as a steady state of preparedness. By quantifying the cost of unpreparedness, our results provide a measurable rationale for European allies to embark on a gradual de-risking trajectory rather than waiting for a much more costly abrupt shock trigger dictated by geopolitical events."
    },
    {
        "text":"With the widespread application of machine learning in financial risk management, conventional wisdom suggests that longer training periods and more feature variables contribute to improved model performance. This paper, focusing on mortgage default prediction, empirically discovers a phenomenon that contradicts traditional knowledge: in time series prediction, increased training data timespan and additional non-critical features actually lead to significant deterioration in prediction effectiveness. Using Fannie Mae's mortgage data, the study compares predictive performance across different time window lengths (2012-2022) and feature combinations, revealing that shorter time windows (such as single-year periods) paired with carefully selected key features yield superior prediction results. The experimental results indicate that extended time spans may introduce noise from historical data and outdated market patterns, while excessive non-critical features interfere with the model's learning of core default factors. This research not only challenges the traditional 'more is better' approach in data modeling but also provides new insights and practical guidance for feature selection and time window optimization in financial risk prediction."
    },
    {
        "text":"Autonomous Vehicles (AVs) represent a transformative advancement in the transportation industry. These vehicles have sophisticated sensors, advanced algorithms, and powerful computing systems that allow them to navigate and operate without direct human intervention. However, AVs' systems still get overwhelmed when they encounter a complex dynamic change in the environment resulting from an accident or a roadblock for maintenance. The advanced features of Sixth Generation (6G) technology are set to offer strong support to AVs, enabling real-time data exchange and management of complex driving maneuvers. This paper proposes a Multi-Agent Reinforcement Learning (MARL) framework to improve AVs' decision-making in dynamic and complex Intelligent Transportation Systems (ITS) utilizing 6G-V2X communication. The primary objective is to enable AVs to avoid roadblocks efficiently by changing lanes while maintaining optimal traffic flow and maximizing the mean harmonic speed. To ensure realistic operations, key constraints such as minimum vehicle speed, roadblock count, and lane change frequency are integrated. We train and test the proposed MARL model with two traffic simulation scenarios using the SUMO and TraCI interface. Through extensive simulations, we demonstrate that the proposed model adapts to various traffic conditions and achieves efficient and robust traffic flow management. The trained model effectively navigates dynamic roadblocks, promoting improved traffic efficiency in AV operations with more than 70% efficiency over other benchmark solutions."
    },
    {
        "text":"We investigate the joint admission control and discrete-phase multicast beamforming design for integrated sensing and communications (ISAC) systems, where sensing and communications functionalities have different hierarchies. Specifically, the ISAC system first allocates resources to the higher-hierarchy functionality and opportunistically uses the remaining resources to support the lower-hierarchy one. This resource allocation problem is a nonconvex mixed-integer nonlinear program (MINLP). We propose an exact mixed-integer linear program (MILP) reformulation, leading to a globally optimal solution. In addition, we implemented three baselines for comparison, which our proposed method outperforms by more than 39%."
    },
    {
        "text":"This paper proposes a novel approach for designing functional observers for nonlinear systems, with linear error dynamics and assignable poles. Sufficient conditions for functional observability are first derived, leading to functional relationships between the Lie derivatives of the output to be estimated and the ones of the measured output. These are directly used in the proposed design of the functional observer. The functional observer is defined in differential input-output form, satisfying an appropriate invariance condition that emerges from the state-space invariance conditions of the literature. A concept of functional observer index is also proposed, to characterize the lowest feasible order of functional observer with pole assignment. Two chemical reactor applications are used to illustrate the proposed approach."
    },
    {
        "text":"This work describes our group's submission to the PROCESS Challenge 2024, with the goal of assessing cognitive decline through spontaneous speech, using three guided clinical tasks. This joint effort followed a holistic approach, encompassing both knowledge-based acoustic and text-based feature sets, as well as LLM-based macrolinguistic descriptors, pause-based acoustic biomarkers, and multiple neural representations (e.g., LongFormer, ECAPA-TDNN, and Trillson embeddings). Combining these feature sets with different classifiers resulted in a large pool of models, from which we selected those that provided the best balance between train, development, and individual class performance. Our results show that our best performing systems correspond to combinations of models that are complementary to each other, relying on acoustic and textual information from all three clinical tasks."
    },
    {
        "text":"Speaker-attributed automatic speech recognition (ASR) in multi-speaker environments remains a significant challenge, particularly when systems conditioned on speaker embeddings fail to generalize to unseen speakers. In this work, we propose Diarization-Conditioned Whisper (DiCoW), a novel approach to target-speaker ASR that leverages speaker diarization outputs as conditioning information. DiCoW extends the pre-trained Whisper model by integrating diarization labels directly, eliminating reliance on speaker embeddings and reducing the need for extensive speaker-specific training data. Our method introduces frame-level diarization-dependent transformations (FDDT) and query-key biasing (QKb) techniques to refine the model's focus on target speakers while effectively handling overlapping speech. By leveraging diarization outputs as conditioning signals, DiCoW simplifies the workflow for multi-speaker ASR, improves generalization to unseen speakers and enables more reliable transcription in real-world multi-speaker recordings. Additionally, we explore the integration of a connectionist temporal classification (CTC) head to Whisper and demonstrate its ability to improve transcription efficiency through hybrid decoding. Notably, we show that our approach is not limited to Whisper; it also provides similar benefits when applied to the Branchformer model. We validate DiCoW on real-world datasets, including AMI and NOTSOFAR-1 from CHiME-8 challenge, as well as synthetic benchmarks such as Libri2Mix and LibriCSS, enabling direct comparisons with previous methods. Results demonstrate that DiCoW enhances the model's target-speaker ASR capabilities while maintaining Whisper's accuracy and robustness on single-speaker data."
    },
    {
        "text":"Large-Scale Multi-Agent Systems (LS-MAS) consist of several autonomous components, interacting in a non-trivial way, so that the emerging behaviour of the ensemble depends on the individual dynamics of the components and their reciprocal interactions. These models can describe a rich variety of natural systems, as well as artificial ones, characterised by unparalleled scalability, robustness, and flexibility. Indeed, a crucial objective is devising efficient strategies to model and control the spatial behaviours of LS-MAS to achieve specific goals. However, the inherent complexity of these systems and the wide spectrum of their emerging behaviours pose significant challenges. The overarching goal of this thesis is, therefore, to advance methods for modelling, analyzing and controlling the spatial behaviours of LS-MAS, with applications to cellular populations and swarm robotics. The thesis begins with an overview of the existing Literature, and is then organized into two distinct parts. In the context of swarm robotics, Part I deals with distributed control algorithms to spatially organize agents on geometric patterns. The contribution is twofold, encompassing both the development of original control algorithms, and providing a novel formal analysis, which allows to guarantee the emergence of specific geometric patterns. In Part II, looking at the spatial behaviours of biological agents, experiments are carried out to study the movement of microorganisms and their response to light stimuli. This allows the derivation and parametrization of mathematical models that capture these behaviours, and pave the way for the development of innovative approaches for the spatial control of microorganisms. The results presented in the thesis were developed by leveraging formal analytical tools, simulations, and experiments, using innovative platforms and original computational frameworks."
    }
]
